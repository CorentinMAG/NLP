{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2_updated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CorentinMAG/NLP/blob/main/Assignment_2_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWw2cPzcovZz"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Due to**: 9th November, 2020\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Word embeddings, from sparse to dense representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xgH0v9hdTKa"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment we will explore text encoding techniques, spanning from sparse representations, such as bag-of-words, to dense representations.\n",
        "\n",
        "In particular, we will see:\n",
        "\n",
        "*   Building a vocabulary\n",
        "*   Building a word-word co-occurrence matrix\n",
        "*   Defining a similarity metric: cosine similarity\n",
        "*   Embedding visualization and analysis of their semantic properties\n",
        "*   Better sparse representations via PPMI weighting\n",
        "*   Loading pre-trained dense word embeddings (Word2Vec, GloVe)\n",
        "*   Checking out-of-vocabulary (OOV) terms\n",
        "*   Handling OOV terms\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUQAeC3gqKhJ"
      },
      "source": [
        "# Initial Setup\n",
        "\n",
        "First of all, we need to import some useful packages that we will use during this hands-on session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_zjgza4qZYE"
      },
      "source": [
        "# system packages\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "# data and numerical management packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# useful during debugging (progress bars)\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI_8CixDqhcm"
      },
      "source": [
        "# [Part I] Sparse embeddings\n",
        "\n",
        "As you know, working with text inherently requires a conversion step, formally known as embedding, that simply allows us to pass from string-like text to corresponding numerical representation. \n",
        "\n",
        "One of the most notable embedding method is the bag-of-words one (BoW). We simply count the occurrence of each word in the given corpus so as to build some useful data structures (matrices) that may give us some general idea of how the dataset is organized. For example, we can check where a particular word appears or, in a reversed perspective, identify the most common terms in each given document.\n",
        "\n",
        "This type of reasoning is directly related to how meaning is assigned to words. In particular, it is the environment itself, enclosing a word, that gives a specific meaning to it. Thus, we look for numerical encoding methods that reflect such point of view.\n",
        "\n",
        "Before diving into embedding analysis, we need to prepare a dataset and, most importantly, extract a vocabulary!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MmTNaGpv6L_"
      },
      "source": [
        "## Prepare a dataset for experiments\n",
        "\n",
        "We will use the IMDB dataset of previous assignment. As you already know, it is a dataset of 50k sentences used for sentiment analysis. In particular, half of them (25k) is labelled as containing positive sentiment, whereas the remaining half are sentences of negative polarity.\n",
        "\n",
        "Contrarily to first assignment, we will ignore sentiment labels and we will focus only on learning a proper word embedding representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWWAMPeTzfSh"
      },
      "source": [
        "### Download and extraction\n",
        "\n",
        "We start by downloading the dataset and extract it to a folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VphzaMCxZps"
      },
      "source": [
        "from urllib import request\n",
        "import tarfile\n",
        "\n",
        "# Config\n",
        "print(\"Current work directory: {}\".format(os.getcwd()))\n",
        "\n",
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\")\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"Movies.tar.gz\")\n",
        "\n",
        "print(dataset_path)\n",
        "\n",
        "def download_dataset(download_path, url):\n",
        "    if not os.path.exists(download_path):\n",
        "        print(\"Downloading dataset...\")\n",
        "        request.urlretrieve(url, download_path)\n",
        "        print(\"Download complete!\")\n",
        "\n",
        "def extract_dataset(download_path, extract_path):\n",
        "    print(\"Extracting dataset... (it may take a while...)\")\n",
        "    with tarfile.open(download_path) as loaded_tar:\n",
        "        loaded_tar.extractall(extract_path)\n",
        "    print(\"Extraction completed!\")\n",
        "\n",
        "# Download\n",
        "download_dataset(dataset_path, url)\n",
        "\n",
        "# Extraction\n",
        "extract_dataset(dataset_path, dataset_folder)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Naaatxml3idO"
      },
      "source": [
        "Feel free to check the dataset folder content. Usually, the README file is a good starting point (if it exists and its well done, which is not so common!).\n",
        "\n",
        "Just like in the first assignment, we need a high level view of the dataset that is helpful to our needs. Thus, we will encode the dataset into a [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvPbnTfOSvbT"
      },
      "source": [
        "# Config\n",
        "dataset_name = \"aclImdb\"\n",
        "debug = True\n",
        "\n",
        "def encode_dataset(dataset_name, debug=True):\n",
        "    dataframe_rows = []\n",
        "\n",
        "    for split in tqdm(['train', 'test']):\n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            folder = os.path.join(os.getcwd(), \"Datasets\", dataset_name, split, sentiment)\n",
        "            for filename in os.listdir(folder):\n",
        "                file_path = os.path.join(folder, filename)\n",
        "                try:\n",
        "                    if os.path.isfile(file_path):\n",
        "                        # open the file\n",
        "                        with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "                            # read it and extract informations\n",
        "                            text = text_file.read()\n",
        "                            score = filename.split(\"_\")[1].split(\".\")[0]\n",
        "                            file_id = filename.split(\"_\")[0]\n",
        "\n",
        "                            num_sentiment = -1\n",
        "\n",
        "                            if sentiment == \"pos\" : num_sentiment = 1\n",
        "                            elif sentiment == \"neg\" : num_sentiment = 0\n",
        "\n",
        "                            # create single dataframe row\n",
        "                            dataframe_row = {\n",
        "                                \"file_id\": file_id,\n",
        "                                \"score\": score,\n",
        "                                \"sentiment\": num_sentiment,\n",
        "                                \"split\": split,\n",
        "                                \"text\": text\n",
        "                            }\n",
        "\n",
        "                            # print detailed info for the first file\n",
        "                            if debug:\n",
        "                                print(file_path)\n",
        "                                print(filename)\n",
        "                                print(file_id)\n",
        "                                print(text)\n",
        "                                print(score)\n",
        "                                print(sentiment)\n",
        "                                print(split)\n",
        "                                print(dataframe_row)\n",
        "                                debug = False\n",
        "                            dataframe_rows.append(dataframe_row)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print('Failed to process %s. Reason: %s' % (file_path, e))\n",
        "                    sys.exit(0)\n",
        "\n",
        "    folder = os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", dataset_name)\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    # transform the list of rows in a proper dataframe\n",
        "    df = pd.DataFrame(dataframe_rows)\n",
        "    df = df[[\"file_id\",\n",
        "                        \"score\",\n",
        "                        \"sentiment\",\n",
        "                        \"split\",\n",
        "                        \"text\"]]\n",
        "    dataframe_path = os.path.join(folder, dataset_name + \".pkl\")\n",
        "    df.to_pickle(dataframe_path)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Encoding\n",
        "print(\"Encoding dataset...\")\n",
        "df = encode_dataset(dataset_name, debug)\n",
        "print(\"Encoding completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHCroMaDdQdB"
      },
      "source": [
        "### Loading and Visualization\n",
        "\n",
        "The next step is to load the dataset and inspect some of its elements in order to have an idea of the general content. We will use **pandas** library for dataset loading as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9itQUTUA3e_C",
        "outputId": "e7e6f0a5-28a9-4d8e-8083-529a7b110b1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Inspection\n",
        "print(\"Dataset size: {}\".format(df.shape)) # (50000, 5)\n",
        "print(\"Dataset columns: {}\".format(df.columns.values)) # ['file_id', 'score', 'sentiment', 'split', 'text]\n",
        "\n",
        "print(\"Classes distribution:\\n{}\".format(df.sentiment.value_counts())) # [0: 25000, 1: 25000]\n",
        "\n",
        "print(\"Some examples: {}\".format(df.iloc[:5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: (50000, 5)\n",
            "Dataset columns: ['file_id' 'score' 'sentiment' 'split' 'text']\n",
            "Classes distribution:\n",
            "1    25000\n",
            "0    25000\n",
            "Name: sentiment, dtype: int64\n",
            "Some examples:   file_id score  ...  split                                               text\n",
            "0   11915    10  ...  train  But how can you stand to mange a baseball team...\n",
            "1    2187    10  ...  train  I enjoyed the story by itself, but the things ...\n",
            "2   10280    10  ...  train  House of Games is spell binding. It's so nice ...\n",
            "3    6857    10  ...  train  \"Dressed To Kill\", is one of the best thriller...\n",
            "4    7214     7  ...  train  Ira Levin's Deathtrap is one of those mystery ...\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuuACGZs5fLS"
      },
      "source": [
        "Feel free to inspect the dataset as you wish in the following code space!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtGCvKxVDbC4"
      },
      "source": [
        "First of all, we convert the score column to int"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svU6BZUf5jCH"
      },
      "source": [
        "df['score'] = pd.to_numeric(df['score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul0utSp_DoCq"
      },
      "source": [
        "print('some stats :')\n",
        "print(df.describe())\n",
        "print()\n",
        "print('check if we have missing values :')\n",
        "df.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLTV9e8LDrlU"
      },
      "source": [
        "We see that we don't have missing data, each review has a score and a sentiment label. Half of the data are labeled positive and the other half negative. Besides, the average score of our dataset is 5.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-DFkVz-Dus7"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIa1rD2FDxdj"
      },
      "source": [
        "df.groupby(['sentiment','score']).count().drop(['file_id','split'],axis=1).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmtkV6SED0zh"
      },
      "source": [
        "We see in the above graph that :\n",
        "\n",
        "* score 1 and 10 are highly represented ( almost half of the data are labeled 1 or 10 )\n",
        "* score from 1 to 4 are labeled 0 (negative)\n",
        "* score from 7 to 10 are labeld 1 (positive)\n",
        "\n",
        "**So we have an imbalanced distribution in our dataset** (but we don't really care because we will just focus on the embedding part)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1ubygt-D4-m"
      },
      "source": [
        "df.groupby(['score','split']).count().drop(['sentiment','text'],axis=1).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOedjzQUD9Au"
      },
      "source": [
        "For each score, we have approximatly the same amount of data in the training and the test dataset.  \n",
        "Now we will visualize the most frequents words in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSv1pjezECmf"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import collections\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cv = CountVectorizer()\n",
        "bow = cv.fit_transform(df['text'])\n",
        "word_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))\n",
        "word_counter = collections.Counter(word_freq)\n",
        "word_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n",
        "fig, ax = plt.subplots(figsize=(18, 12))\n",
        "sns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHInIaRrEH1P"
      },
      "source": [
        "We see that one of the most common word in the dataset is br, which is an html tag.  \n",
        "We need to get rid of that in the cleaning section !  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTYo05YVeu7e"
      },
      "source": [
        "### [Optional] A quick simplification\n",
        "\n",
        "Since the dataset is quite large, the embedding related methods, such as co-occurrence matrix construction, may take a while or may require ad hoc solutions. For instance, if we consider the whole dataset (50k sentences) the vocabulary should be around 160k terms and we don't have sufficient memory to load a (160k, 160k) co-occurrence matrix.\n",
        "\n",
        "For the purpose of this assignment, we can rely on a small slice of the dataset.\n",
        "In this way, we can get results in small amount of time. Nonetheless, feel free\n",
        "to work with the whole dataset! Suggestions on how to handle this scenario are\n",
        "given below when required.\n",
        "\n",
        "Select the amount of dataset samples you want to keep and re-define the dataset as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyghmotufcFe",
        "outputId": "3e54bec6-1a7f-4473-9650-9b601913eec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "samples_amount = 500\n",
        "\n",
        "# This type of slicing is not mandatory,\n",
        "# but it is sufficient to our purposes\n",
        "np.random.seed(42)\n",
        "random_indexes = np.random.choice(np.arange(df.shape[0]),\n",
        "                                  size=samples_amount,\n",
        "                                  replace=False)\n",
        "\n",
        "df = df.iloc[random_indexes]\n",
        "\n",
        "print('New dataset size: ', df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New dataset size:  (500, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohww_Fda5wR-"
      },
      "source": [
        "## Building the Vocabulary\n",
        "\n",
        "At this point we can build the word vocabulary of our dataset. This information is the first step of any word embedding method: we need to know the set of atomic entities that build up our corpus.\n",
        "\n",
        "**Definition**: a vocabulary is a collection of words occurring in a given dataset. More precisely, each word is recognized and assigned an index.\n",
        "\n",
        "**Example**: Suppose you have the given toy corpus $D$: { \"the cat is on the table\" }\n",
        "\n",
        "As you notice, the dataset is comprised of only one sentence: \"the cat is on the table\". The corresponding vocabulary (a possible one) will be:\n",
        "\n",
        "V = {0: 'the', 1: 'cat', 2: 'is', 3: 'on', 4: 'table'}\n",
        "\n",
        "In this case, indexing follows word order, but it is not mandatory!\n",
        "\n",
        "**Important**: The most important thing to remember is that the vocabulary should always be the same one. Thus, make sure that the vocabulary creation routine always returns the same result!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYOu4FV067O_"
      },
      "source": [
        "### Some Cleaning\n",
        "\n",
        "Before vocabulary creation, we have to do a little bit of text pre-processing so as to avoid spurious data.\n",
        "\n",
        "Pre-processing is always an important step in any machine learning based task, since data quality is one of the crucial factors that lead to better performance. Models, even state-of-the-art ones, hardly achieve satisfying results if the dataset is very noisy.\n",
        "\n",
        "**Types of pre-processing**: there are a lot of pre-processing steps that we can consider, either general or quite task- specific. Here we will rely on very standard and simple methods.\n",
        "\n",
        "*    **Text to lower**: casing usually doesn't affect our task, but in some scenarios, such as part-of-speech tagging, might even be crucial.\n",
        "\n",
        "*    **Replace special characters**: special characters are usually employed as variants of a single character like the spacing symbol ' '. \n",
        "In other cases (dates, etc..) special characters might have a specific meaning and should not be replaced.\n",
        "\n",
        "*    **Text stripping**: it is important to filter out extra spaces to avoid unwanted distinctions between identical words, such as 'apple' and ' apple '.\n",
        "\n",
        "There are a lot of pre-processing techniques, such as number replacing, lemmatization, stemming, spell correction, acronyms merge and so on. If you are interested you can check [here](https://medium.com/swlh/text-normalization-7ecc8e084e31) and [here](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html) some good blogs about the topic.\n",
        "\n",
        "**NOTE**: If you feel like there should be some additional pre-processing, feel free to modify this section as you please! Please, remember to provide additional comments to motivate your changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TLTu0-2JQwi"
      },
      "source": [
        "import re\n",
        "from functools import reduce\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Config\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def lower(text):\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text):\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "  \n",
        "def remove_html_tags(text):\n",
        "  \"\"\"\n",
        "    Replaces html tags with empty string\n",
        "  \"\"\"\n",
        "  return re.sub('<[^>]*>',' ',text)\n",
        "\n",
        "def remove_digits(text):\n",
        "  \"\"\"\n",
        "    Remove all digits from reviews\n",
        "  \"\"\"\n",
        "  return re.sub('[0-9]+',' ',text)\n",
        "\n",
        "def filter_out_uncommon_symbols(text):\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "\n",
        "def strip_text(text):\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "PREPROCESSING_PIPELINE = [\n",
        "                          lower,\n",
        "                          remove_html_tags,\n",
        "                          remove_digits,\n",
        "                          replace_special_characters,\n",
        "                          filter_out_uncommon_symbols,\n",
        "                          remove_stopwords,\n",
        "                          strip_text\n",
        "                          ]\n",
        "\n",
        "# Anchor method\n",
        "\n",
        "def text_prepare(text, filter_methods=None):\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "\n",
        "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "# Pre-processing\n",
        "\n",
        "print('Pre-processing text...')\n",
        "\n",
        "print()\n",
        "print('[Debug] Before:\\n{}'.format(df.text[:3]))\n",
        "print()\n",
        "\n",
        "# Replace each sentence with its pre-processed version\n",
        "df['text'] = df['text'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "print('[Debug] After:\\n{}'.format(df.text[:3]))\n",
        "print()\n",
        "\n",
        "print(\"Pre-processing completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LFirfD3Edka"
      },
      "source": [
        "I have added two preprocessed functions:\n",
        "\n",
        "* one to remove html tags because it's some kind of noise\n",
        "* one to remove all digits because it's more convenient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw4-mEf93d5f"
      },
      "source": [
        "cv = CountVectorizer()\n",
        "bow = cv.fit_transform(df['text'])\n",
        "word_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))\n",
        "word_counter = collections.Counter(word_freq)\n",
        "word_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n",
        "fig, ax = plt.subplots(figsize=(18, 12))\n",
        "sns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RLXy7tp3gFn"
      },
      "source": [
        "We see now that all \"useless\" words have been removed.  \n",
        "Other nice preprocessing steps would be to correct all misspell words, but that's totaly outside my scope, and to use lemmatization and/or stemming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YphBz_iaMRm0"
      },
      "source": [
        "### **Vocabulary Creation**\n",
        "\n",
        "We are now ready to create the vocabulary! This task is up to you! Complete the below function and remember to follow mentioned requirements.\n",
        "\n",
        "FYI, since the text has been pre-processed, space splitting should work correctly. \n",
        "\n",
        "Bare in mind that some packages offers tools for automatic vocabulary creation, such as Keras (check [keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)). \n",
        "\n",
        "**NOTE**: It is not mandatory to use the keras Tokenizer, we mention it so that you know there exist tools specific for this step.\n",
        "\n",
        "**NOTE**: In this case, the vocabulary will start from index equal to 1, since the 0 slot is reserved to padding token. In order to pass vocabulary evaluation, you have to re-scale the vocabulary such that the first index is 0.\n",
        "\n",
        "**NOTE**: If you are using the keras Tokenizer, remember to use its method <code> texts_to_sequences </code> for splitting, otherwise you might find terms that are not in the vocabulary! Please, check also its constructor argument <code> filters </code> since it defines a pre-processing regexp.\n",
        "\n",
        "This easy task is just to let you know that is important to check the built vocabulary just to make sure everything is ok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRs7m-SxMq0n",
        "outputId": "a13096a9-f571-433a-bfaa-57040b8c5cf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "# Function definition\n",
        "def build_vocabulary(df):\n",
        "    \"\"\"\n",
        "    Given a dataset, builds the corresponding word vocabulary.\n",
        "\n",
        "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
        "    :return:\n",
        "      - word vocabulary: vocabulary index to word\n",
        "      - inverse word vocabulary: word to vocabulary index\n",
        "      - word listing: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "    idx_to_word={}\n",
        "    word_to_idx={}\n",
        "    word_listing=[]\n",
        "    i=0\n",
        "    for index,row in df.iterrows():\n",
        "      word_list = row['text'].split(' ')\n",
        "      for w in word_list:\n",
        "        w = w.strip()\n",
        "        if w not in word_listing:\n",
        "          word_listing.append(w)\n",
        "          word_to_idx[w] = i\n",
        "          idx_to_word[i] = w\n",
        "          i+=1\n",
        "    return idx_to_word,word_to_idx,word_listing\n",
        " \n",
        "# Testing\n",
        "idx_to_word, word_to_idx, word_listing = build_vocabulary(df)\n",
        "\n",
        "print('[Debug] Index -> Word vocabulary size: {}'.format(len(idx_to_word)))\n",
        "print('[Debug] Word -> Index vocabulary size: {}'.format(len(word_to_idx)))\n",
        "\n",
        "print('[Debug] Some words: {}'.format([(idx_to_word[idx], idx) for idx in np.arange(10) + 1]))\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def evaluate_vocabulary(idx_to_word, word_to_idx, word_listing, df, check_default_size=False):\n",
        "\n",
        "    # Check size\n",
        "    print(\"[Vocabulary Evaluation] Size checking...\")\n",
        "\n",
        "    assert len(idx_to_word) == len(word_to_idx)\n",
        "    assert len(idx_to_word) == len(word_listing)\n",
        "\n",
        "    # Check content\n",
        "    print(\"[Vocabulary Evaluation] Content checking...\")\n",
        "\n",
        "    for i in tqdm(range(0, len(idx_to_word))):\n",
        "        assert idx_to_word[i] in word_to_idx\n",
        "        assert word_to_idx[idx_to_word[i]] == i\n",
        "\n",
        "    # Check consistency\n",
        "    print(\"[Vocabulary Evaluation] Consistency checking...\")\n",
        "\n",
        "    _, _, first_word_listing = build_vocabulary(df)\n",
        "    _, _, second_word_listing = build_vocabulary(df)\n",
        "    assert first_word_listing == second_word_listing\n",
        "\n",
        "    # Check toy example\n",
        "    print(\"[Vocabulary Evaluation] Toy example checking...\")\n",
        "    toy_df = pd.DataFrame.from_dict({\n",
        "        'text': [\"all that glitters is not gold\", \"all in all i like this assignment\"]\n",
        "    })\n",
        "    _, _, toy_word_listing = build_vocabulary(toy_df)\n",
        "    toy_valid_vocabulary = set(' '.join(toy_df.text.values).split())\n",
        "    assert set(toy_word_listing) == toy_valid_vocabulary\n",
        "\n",
        "\n",
        "print(\"Vocabulary evaluation...\")\n",
        "evaluate_vocabulary(idx_to_word, word_to_idx, word_listing, df)\n",
        "print(\"Evaluation completed!\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13289/13289 [00:00<00:00, 1867648.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Debug] Index -> Word vocabulary size: 13289\n",
            "[Debug] Word -> Index vocabulary size: 13289\n",
            "[Debug] Some words: [('one', 1), ('word', 2), ('pretty', 3), ('much', 4), ('sums', 5), ('whole', 6), ('film', 7), ('everything', 8), ('cinematography', 9), ('directing', 10)]\n",
            "Vocabulary evaluation...\n",
            "[Vocabulary Evaluation] Size checking...\n",
            "[Vocabulary Evaluation] Content checking...\n",
            "[Vocabulary Evaluation] Consistency checking...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Vocabulary Evaluation] Toy example checking...\n",
            "Evaluation completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSO2NA5ZYnz_"
      },
      "source": [
        "Feel free to inspect the vocabulary! To this purpose, you can use the following code space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT4oB5PcYyd1"
      },
      "source": [
        "print(f'number of tokens : {len(idx_to_word)}')\n",
        "for i in range(0,5):\n",
        "  print(i,idx_to_word[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pUTYnmcFQct"
      },
      "source": [
        "### **Save the vocabulary**\n",
        "\n",
        "Generally speaking, it is a good idea to save the dictionary in clear format. In this way you can quickly check for errors or useful words.\n",
        "\n",
        "In this case, we will save the vocabulary dictionary in JSON format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg41avgpFlFJ"
      },
      "source": [
        "!pip install simplejson\n",
        "import simplejson as sj\n",
        "\n",
        "vocab_path = os.path.join(os.getcwd(), 'Datasets', dataset_name, 'vocab.json')\n",
        "\n",
        "print(\"Saving vocabulary to {}\".format(vocab_path))\n",
        "with open(vocab_path, mode='w') as f:\n",
        "    sj.dump(word_to_idx, f, indent=4)\n",
        "print(\"Saving completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrWb80lyY629"
      },
      "source": [
        "## Building the Co-occurence Matrix\n",
        "\n",
        "As we said at the beginning, embedding methods are based on the principle that similar words will be used in similar contexts. Thus, context information is crucial to determine the meaning of a word.\n",
        "\n",
        "One basic approach, which falls under the category of sparse representations, is the **co-occurrence matrix**: for each word in the vocabulary we count the number of times each other word appears within the same context window. A simple example is given by image below.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1UknGoYvIBBA7ytkSlqm1NhF_lHt0iOwT)\n",
        "\n",
        "In particular, the context window defines our notion of word context. Consider the following example:\n",
        "\n",
        "<h3><center> The cat is on the table </center></h3>\n",
        "\n",
        "We have to consider each word in the sentence and for each we have count words within the context window. Suppose a window of size 2, then we have for the word 'cat':\n",
        "\n",
        "Current word: cat\n",
        "\n",
        "Context words: [the, is, on]\n",
        "\n",
        "Notice how we consider $W$ words back and ahead of current word, where $W$ is the window size.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's define the simplest version of a **co-occurrence matrix** based on word counting.\n",
        "\n",
        "---\n",
        "\n",
        "**Small dataset case**: If you selected a small slice of the dataset, you should have a vocabulary size that we can afford in terms of memory demand. Thus, you can easily instantiate the co-occurrence matrix and populate it iteratively.\n",
        "\n",
        "---\n",
        "\n",
        "**Large dataset case**: We have to work with sparse matrices due to the high vocabulary size and to the low amount of non-zero word counts. To this end, the [Scipy package](https://docs.scipy.org/doc/scipy/reference/sparse.html) allows us to easily define sparse matrices that can be converted ot numpy arrays (if we can).\n",
        "\n",
        "**Suggestion**: The simplest way to build the co-occurrence matrix is via an incremental approach: we loop through dataset sentences, split into words and then count co-occurrences within the given window frame. Generally, combining this approach with sparse matrices is not so efficient (yet possible). However, Scipy offers [$\\texttt{lil_matrix}$](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html#scipy.sparse.lil_matrix) sparse format that is suitable to this case. Anyway, check out other sparse formats, such as [$\\texttt{csr_matrix}$](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix), and the corresponding building methods.\n",
        "\n",
        "Working with $\\texttt{lil_matrix}$ might take $\\sim 1h$ of time to build the whole dataset co-occurrence matrix. It is also possibile to work with $\\texttt{csr_matrix}$ but the approach is more complex (check the last example of the corresponding documentation page).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS5H65sh52b9"
      },
      "source": [
        "import scipy.sparse    # defines several types of efficient sparse matrices\n",
        "import zipfile\n",
        "import gc\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# Function definition\n",
        "\n",
        "def co_occurrence_count(df, idx_to_word, word_to_idx, window_size=4):\n",
        "    \"\"\"\n",
        "    Builds word-word co-occurrence matrix based on word counts.\n",
        "\n",
        "    :param df: pre-processed dataset (pandas.DataFrame)\n",
        "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "\n",
        "    :return\n",
        "      - co_occurrence symmetric matrix of size |V| x |V| (|V| = vocabulary size)\n",
        "    \"\"\"\n",
        "\n",
        "    co_occurence_matrix = scipy.sparse.lil_matrix((len(idx_to_word),len(idx_to_word)),dtype=np.float64)\n",
        "    for review in df['text']:\n",
        "      word_list = review.split(' ')\n",
        "      for i,w in enumerate(word_list):\n",
        "        centered_word = w\n",
        "        right_outside_words = word_list[i+1:i+window_size+1]\n",
        "        for ow in right_outside_words:\n",
        "          co_occurence_matrix[word_to_idx[centered_word],word_to_idx[ow]]+=1\n",
        "        if i > window_size:\n",
        "          left_outside_words = word_list[i-1:i-1-window_size:-1]\n",
        "        elif i == 0:\n",
        "          left_outside_words = []\n",
        "        else:\n",
        "          left_outside_words = word_list[i-1::-1]\n",
        "        for ow in left_outside_words:\n",
        "          co_occurence_matrix[word_to_idx[centered_word],word_to_idx[ow]]+=1\n",
        "    return co_occurence_matrix\n",
        "\n",
        "\n",
        "# Testing\n",
        "window_size = 4\n",
        "\n",
        "# Clean RAM before re-running this code snippet to avoid session crash\n",
        "if 'co_occurrence_matrix' in globals():\n",
        "    del co_occurrence_matrix\n",
        "    gc.collect()\n",
        "    time.sleep(10.)\n",
        "\n",
        "print(\"Building co-occurrence count matrix... (it may take a while...)\")\n",
        "co_occurrence_matrix = co_occurrence_count(df, idx_to_word, word_to_idx, window_size)\n",
        "print(\"Building completed!\")\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_toy_data(benchmark_path):\n",
        "    toy_data_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark.zip')\n",
        "    toy_data_url_id = \"1z8qp034utvW7kv-9Q_TACJv3_sdCzkZg\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(benchmark_path):\n",
        "        os.makedirs(benchmark_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading co-occurrence count matrix benchmark data...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download complete!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(benchmark_path)\n",
        "        print(\"Extraction complete!\")\n",
        "\n",
        "def evaluate_co_occurrence_matrix(matrix):\n",
        "    is_sparse = False\n",
        "\n",
        "    if hasattr(scipy.sparse, type(matrix).__name__):\n",
        "        print(\"Detected sparse co-occurrence matrix!\")\n",
        "        is_sparse = True\n",
        "\n",
        "    # Check symmetry\n",
        "    print(\"[Co-occurrence count matrix Evaluation] Symmetry checking...\")\n",
        "    if is_sparse:\n",
        "        assert (matrix != matrix.transpose()).nnz == 0\n",
        "    else:\n",
        "        assert np.equal(matrix, matrix.transpose()).all()\n",
        "\n",
        "    # Check toy example\n",
        "    print(\"[Co-occurrence count matrix Evaluation] Toy example checking...\")\n",
        "    toy_df = pd.DataFrame.from_dict({\n",
        "        'text': [\"all that glitters is not gold\",\n",
        "                 \"all in all i like this assignment\"],\n",
        "    })\n",
        "    benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n",
        "    toy_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark')\n",
        "    download_toy_data(benchmark_path)\n",
        "\n",
        "    toy_idx_to_word = np.load(os.path.join(toy_path, 'toy_idx_to_word.npy'), allow_pickle=True).item()\n",
        "    toy_word_to_idx = np.load(os.path.join(toy_path, 'toy_word_to_idx.npy'), allow_pickle=True).item()\n",
        "\n",
        "    toy_matrix = co_occurrence_count(toy_df, toy_idx_to_word, toy_word_to_idx, window_size=1)\n",
        "    toy_valid_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_count.npy'))\n",
        "\n",
        "    if is_sparse:\n",
        "        assert np.equal(toy_matrix.todense(), toy_valid_matrix).all()\n",
        "    else:\n",
        "        assert np.equal(toy_matrix, toy_valid_matrix).all()\n",
        "\n",
        "\n",
        "print(\"Evaluating co-occurrence matrix\")\n",
        "evaluate_co_occurrence_matrix(co_occurrence_matrix)\n",
        "print(\"Evaluation completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So0i7CWy-fAW"
      },
      "source": [
        "### **Got Stuck?**\n",
        "\n",
        "If you are stuck, but still want to try out following sections, you can experiment with a valid co-occurrence matrix provided by us as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJWDWacd-vrG"
      },
      "source": [
        "# benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n",
        "# valid_data_benchmark_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark', \"{}.npy\")\n",
        "\n",
        "# download_toy_data(benchmark_path)\n",
        "\n",
        "# co_occurrence_matrix = np.load(valid_data_benchmark_path.format('valid_co-occurrence_matrix_count'))\n",
        "# idx_to_word = np.load(valid_data_benchmark_path.format('valid_idx_to_word'), allow_pickle=True).item()\n",
        "# word_to_idx = np.load(valid_data_benchmark_path.format('valid_word_to_idx'), allow_pickle=True).item()\n",
        "# word_listing = np.load(valid_data_benchmark_path.format('valid_word_listing'))\n",
        "\n",
        "# print('Co-occurrence matrix shape: ', co_occurrence_matrix.shape)\n",
        "# print('Index -> word vocabulary size: ', len(idx_to_word))\n",
        "# print('Word -> index vocabulary size: ', len(word_to_idx))\n",
        "# print('Word listing size: ', len(word_listing))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H10Eqj5-CyZv"
      },
      "source": [
        "## Embedding Visualization\n",
        "\n",
        "The next step is to visualize our sparse word embeddings in a lower dimensional space (2D) in order to have an idea of the meaning attributed to each word.\n",
        "\n",
        "**How?** Well, there are some dimensionality reduction techniques that we might employ. We will explore SVD and t-SNE methods, without delving into technical details since they are not arguments of this NLP course.\n",
        "\n",
        "**SVD Memo**: SVD stands for **Singular Value Decomposition** and is a kind of generalized **Principal Components Analysis** (PCA) and focuses on selecting the top **k** principal components. For more info, [here](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf) you can find a brief tutorial.\n",
        "\n",
        "**t-SNE Memo**: t-SNE stands for **t-Distributed Stochastic Neighbour Embedding** and is an unsupervised non-linear technique. The non-linearity is one major point of difference with PCA. Additionally, it preserves small pairwise distance (or local similarities), whereas PCA aims to preserve large pairwise distances in order to maximize variance. The basic idea of t-SNE is to compute a similarity measure between a pair of instances both at high and low dimensional space and optimize these two similarities via a cost function. Properly using t-SNE is a bit tricky, a well recommended reading is one of the [author's blog](https://lvdmaaten.github.io/tsne/).\n",
        "\n",
        "**Note**: We strongly suggest you to play with the window size and check if there are some notable differences. Generally, a small window size reflects syntactic properties, while a large window size captures semantic ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNzFd1-4JBvS"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function definition\n",
        "\n",
        "def visualize_embeddings(embeddings, word_annotations=None, word_to_idx=None):\n",
        "    \"\"\"\n",
        "    Plots given reduce word embeddings (2D).\n",
        "    Users can highlight specific words (word_annotations list) in order to better\n",
        "    analyse the effectiveness of the embedding method.\n",
        "\n",
        "    :param embeddings: word embedding matrix of shape (words, 2) retrieved via a\n",
        "                       dimensionality reduction technique.\n",
        "    :param word_annotations: list of words to be annotated.\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
        "\n",
        "    if word_annotations:\n",
        "        print(\"Annotating words: {}\".format(word_annotations))\n",
        "\n",
        "        word_indexes = []\n",
        "        for word in word_annotations:\n",
        "            word_index = word_to_idx[word]\n",
        "            word_indexes.append(word_index)\n",
        "\n",
        "        word_indexes = np.array(word_indexes)\n",
        "\n",
        "        other_embeddings = embeddings[np.setdiff1d(np.arange(embeddings.shape[0]), word_indexes)]\n",
        "        target_embeddings = embeddings[word_indexes]\n",
        "\n",
        "        ax.scatter(other_embeddings[:, 0], other_embeddings[:, 1], alpha=0.1, c='blue')\n",
        "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='red')\n",
        "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='r', s=1000)\n",
        "\n",
        "        for word, word_index in zip(word_annotations, word_indexes):\n",
        "            word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n",
        "            ax.annotate(word, xy=(word_x, word_y))\n",
        "\n",
        "    else:\n",
        "        ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n",
        "\n",
        "    # Set proper axis limit range\n",
        "    # We avoid outliers ruining the visualization if they are quite far away\n",
        "    xmin_quantile = np.quantile(embeddings[:, 0], q=0.01)\n",
        "    xmax_quantile = np.quantile(embeddings[:, 0], q=0.99)\n",
        "\n",
        "    ymin_quantile = np.quantile(embeddings[:, 1], q=0.01)\n",
        "    ymax_quantile = np.quantile(embeddings[:, 1], q=0.99)\n",
        "\n",
        "    ax.set_xlim(xmin_quantile, xmax_quantile)\n",
        "    ax.set_ylim(ymin_quantile, ymax_quantile)\n",
        "\n",
        "\n",
        "def reduce_SVD(embeddings):\n",
        "    \"\"\"\n",
        "    Applies SVD dimensionality reduction.\n",
        "\n",
        "    :param embeddings: word embedding matrix of shape (words, dim). In the case\n",
        "                       of a word-word co-occurrence matrix the matrix shape would\n",
        "                       be (words, words).\n",
        "\n",
        "    :return\n",
        "        - 2-dimensional word embedding matrix of shape (words, 2)\n",
        "    \"\"\"\n",
        "  \n",
        "    print(\"Running SVD reduction method...\")\n",
        "    svd = TruncatedSVD(n_components=2, n_iter=10, random_state=42)\n",
        "    reduced = svd.fit_transform(embeddings)\n",
        "    print(\"SVD reduction completed!\")\n",
        "\n",
        "    return reduced\n",
        "\n",
        "# Note: this method may take a while\n",
        "def reduce_tSNE(embeddings):\n",
        "    \"\"\"\n",
        "    Applies t-SNE dimensionality reduction.\n",
        "\n",
        "    :param embeddings: word embedding matrix of shape (words, dim). In the case\n",
        "                       of a word-word co-occurrence matrix the matrix shape would\n",
        "                       be (words, words).\n",
        "\n",
        "    :return\n",
        "        - 2-dimensional word embedding matrix of shape (words, 2)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Running t-SNE reduction method... (it may take a while...)\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, n_iter=1000, metric='cosine', n_jobs=2)\n",
        "    reduced = tsne.fit_transform(embeddings)\n",
        "    print(\"t-SNE reduction completed!\")\n",
        "    print(reduced.shape)\n",
        "\n",
        "    return reduced\n",
        "\n",
        "# Testing\n",
        "\n",
        "# Feel free to play with word_annotations argument!\n",
        "# Some suggestions: stopwords (if not removed), nouns, adjectives\n",
        "# Check the saved dictionary!\n",
        "\n",
        "# SVD\n",
        "reduced_SVD = reduce_SVD(co_occurrence_matrix)\n",
        "visualize_embeddings(reduced_SVD, ['good', 'love', 'beautiful','movie','film'], word_to_idx)\n",
        "\n",
        "# t-SNE\n",
        "# Note: this method may take a while (just relax :-))\n",
        "reduced_tSNE = reduce_tSNE(co_occurrence_matrix)\n",
        "visualize_embeddings(reduced_tSNE, ['good', 'love', 'beautiful','enjoy','incredible','bad','awful','ugly','horrible','movie','film'], word_to_idx)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYiMKNFp6avp"
      },
      "source": [
        "Words seem very close to each others. That a bit strange.  \n",
        "Indeed, the raw frequency is very skewed and not very discriminative, as we can see.  \n",
        "Lots of the words I have plot are adjectives. And adjectives tends to have a similar context .(we usually say he is horrible, he is beautiful..) so, according to the raw frequency count, all adjectives are quite similar.\n",
        "If we try a larger window length, words should be more scatered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjI9eByS8AKa"
      },
      "source": [
        "Feel free to play with visualization!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg58nCFY0_zs"
      },
      "source": [
        "I will visualize words `king`, `woman`, `queen` and `man`.  \n",
        " I expect :\n",
        "* `king` to be far from `woman` but close to `man` and `queen`\n",
        "* `queen` to be far from `man` but close to `woman` and `king`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-6FTsUs02KU"
      },
      "source": [
        "visualize_embeddings(reduced_tSNE, ['king','woman','queen','man'], word_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7wuYnIX3w7B"
      },
      "source": [
        "That doesn't really meet our expectation! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxPTZE_U4qUk"
      },
      "source": [
        "### Embedding properties\n",
        "\n",
        "Visualization can give us a rough idea of how word embeddings are organized and if some semantic properties are reflected in the numerical dimensional space. For example, are synonyms close together? Ideally, if the dataset is big enough, we should see similar vector embeddings since synonyms usually have similar contexts.\n",
        "\n",
        "**How to do that?** We could highlight target words in the visualization step and check if our expectations are met. For instance, synonyms should be close together. However, this method is rather inaccurate and time-consuming (dimensionality reduction is not a perfect mapping). Thus, we need some sort of similarity metric that is independent of the vector dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrhbF-lEESyW"
      },
      "source": [
        "#### **Cosine Similarity**\n",
        "\n",
        "Let us now consider again the matrix obtained using <code>co_occurrence_count</code>. \n",
        "Since we want to meaure how two word vectors are far apart, a naive solution would involve computing the dot product of the two vectors. However, this metric will give higher similarity either to longer vectors or to vectors that have higher counts.\n",
        "\n",
        "A better metric is **cosine similarity** which is just a normalized dot product.\n",
        "\n",
        "$s(p, q) = \\frac{p \\, \\cdot \\, q}{||p|| \\, \\cdot \\, ||q||}$\n",
        "\n",
        "where $s(p, q) \\in [-1, 1] $, since it computes the cosine of the angle between the two vectors. Intuitively, we are bringing vectors down to the d-dimensional unit sphere (d is the vocab size) and then computing their distance (in 2D space we will have a circle).\n",
        "\n",
        "Now, write down the cosine similarity formula so that we can proceed testing word embedding properties!\n",
        "\n",
        "**NOTE**: Since we are working with matrices, we will ask you to define a cosine similarity function that works with matrices. Extending to matrices is quite easy if you think them as lists of vectors.\n",
        "\n",
        "**NOTE**: It is permitted to use functions of existing packages (e.g. sci-kit learn). This is mainly for efficiency motivation. We are not going to discriminate between such solutions and the ones that manually implement the cosine similarity metric (since it is not the main objective of the assignment).\n",
        "\n",
        "**WHAT YOU HAVE TO DO**: First of all, try to manually define the cosine similarity operation. If your implementation is correct but not so efficient, you can define a separate cosine similarity function that leverages implementations of existing packages like sci-kit learn. Here we want to verify if you understand the metric, but at the same time we don't want to penalize you if you are not as efficient as possible (although it is one factor that it is important to not forget when coding in general)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wufaiAs78E1E"
      },
      "source": [
        "# Function definition\n",
        "\n",
        "def cosine_similarity(p, q, transpose_p=False, transpose_q=False):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity of two d-dimensional matrices\n",
        "\n",
        "    :param p: d-dimensional vector (np.ndarray) of shape (p_samples, d)\n",
        "    :param q: d-dimensional vector (np.ndarray) of shape (q_samples, d)\n",
        "    :param transpose_p: whether to transpose p or not\n",
        "    :param transpose_q: whether to transpose q or not\n",
        "\n",
        "    :return\n",
        "        - cosine similarity matrix S of shape (p_samples, q_samples)\n",
        "          where S[i, j] = s(p[i], q[j])\n",
        "    \"\"\"\n",
        "\n",
        "    # If it is a vector, consider it as a single sample matrix\n",
        "    if len(p.shape) == 1:\n",
        "        p = p.reshape(1, -1)\n",
        "    if len(q.shape) == 1:\n",
        "        q = q.reshape(1, -1)\n",
        "\n",
        "    # we check if it is a sparce matrix or not\n",
        "    if type(q) == scipy.sparse.lil.lil_matrix:\n",
        "      q = q.todense()\n",
        "    if type(p) == scipy.sparse.lil.lil_matrix:\n",
        "      p = p.todense()\n",
        "\n",
        "    q_samples = q.shape[0]\n",
        "    p_samples = p.shape[0]\n",
        "    if transpose_q:\n",
        "      d = p.dot(q.T)\n",
        "      normp = np.linalg.norm(p,2,axis=1).reshape((p_samples,1))\n",
        "      normq = np.linalg.norm(q,2,axis=1)\n",
        "    elif transpose_p:\n",
        "      d = q.dot(p.T)\n",
        "      normp = np.linalg.norm(p,2,axis=1)\n",
        "      normq = np.linalg.norm(q,2,axis=1).reshape((q_samples,1))\n",
        "    else:\n",
        "      d = q.dot(p)\n",
        "      normp = np.linalg.norm(p,2,axis=1).reshape((p_samples,1))\n",
        "      normq = np.linalg.norm(q,2,axis=1)\n",
        "    \n",
        "    norm = normp * normq\n",
        "    S = d / norm\n",
        "    return S if type(S) == np.ndarray else S.A\n",
        "\n",
        "# Testing\n",
        "\n",
        "print(\"Computing similarity matrix...\")\n",
        "similarity_matrix = cosine_similarity(co_occurrence_matrix,\n",
        "                                      co_occurrence_matrix,\n",
        "                                      transpose_q=True)\n",
        "print(\"Similarity completed!\")\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def sparse_allclose(a, b, rtol=1e-5, atol = 1e-8):\n",
        "    c = np.abs(np.abs(a - b) - rtol * np.abs(b))\n",
        "    return c.max() <= atol\n",
        "\n",
        "def evaluate_cosine_similarity(similarity_matrix):\n",
        "\n",
        "    # Vector similarity\n",
        "    print('[Cosine similarity Evaluation] Vector similarity check...')\n",
        "\n",
        "    p = np.array([5., 6., 0.3, 1.])\n",
        "    q = np.array([50., 6., 0., 0.])\n",
        "    assert np.allclose([[0.72074324]], cosine_similarity(p, q, transpose_q=True))\n",
        "\n",
        "    # Matrix similarity\n",
        "    print('[Cosine similarity Evaluation] Matrix similarity check...')\n",
        "\n",
        "    toy_matrix = np.array([5., 6., 0.3, 1.,\n",
        "                           50., 6., 0., 0.,\n",
        "                           0., 100., 20., 4.]).reshape(3, 4)\n",
        "    true_matrix = np.array([1., 0.72074324, 0.75852259,\n",
        "                            0.72074324, 1., 0.11674173,\n",
        "                            0.75852259, 0.11674173, 1.]).reshape(3, 3)\n",
        "    proposed_matrix = cosine_similarity(toy_matrix, toy_matrix, transpose_q=True)\n",
        "    \n",
        "    assert np.allclose(proposed_matrix, true_matrix)\n",
        "\n",
        "    # There might be some numerical error that invalidates the np.equal check\n",
        "    assert np.allclose(proposed_matrix, proposed_matrix.transpose())\n",
        "\n",
        "    # Check symmetry\n",
        "    print(\"[Cosine similarity Evaluation] Symmetry checking...\")\n",
        "\n",
        "    is_sparse = False\n",
        "\n",
        "    if hasattr(scipy.sparse, type(similarity_matrix).__name__):\n",
        "        print(\"Detected sparse cosine similarity matrix!\")\n",
        "        is_sparse = True\n",
        "\n",
        "    if is_sparse:\n",
        "        try:\n",
        "            assert (similarity_matrix != similarity_matrix.transpose()).nnz == 0\n",
        "        except AssertionError:\n",
        "            assert sparse_allclose(similarity_matrix, similarity_matrix.transpose())\n",
        "    else:\n",
        "        # There might be some numerical error that invalidates the np.equal check\n",
        "        assert np.allclose(similarity_matrix, similarity_matrix.transpose())\n",
        "\n",
        "print('Evaluating cosine similarity...')\n",
        "evaluate_cosine_similarity(similarity_matrix)\n",
        "print('Evaluation completed!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPnB9NrHETC8"
      },
      "source": [
        "#### **[Let's play!] Synonyms and Antonyms**\n",
        "\n",
        "Look for some words and provide a possible explanation of achieved results according to cosine similarity metric (you can also refer to previous visualization step).\n",
        "\n",
        "* Synonym pair: (w1, w2) such that w1 and w2 are synonyms\n",
        "* Antonyms pair: (w1, w2) such that w1 and w2 are antonyms\n",
        "* Synonym-Antonym triplet: (w1, w2, w3) such that w1 and w2 are synonyms and w1 and w3 are antonyms\n",
        "\n",
        "You can also support your answer by checking word contexts of selected words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXJpJnQP-j_S"
      },
      "source": [
        "We can begin computing similarities by saying that 2 words that have the same context are similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3lT1fhm-t-K"
      },
      "source": [
        "import seaborn as sns\n",
        "ax1 = sns.heatmap(similarity_matrix[:30,:30],linewidths=2,cmap=\"plasma\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5ybwEQFAUCN"
      },
      "source": [
        "Thanks to this heatmap, we can see if 2 words are highly correlated or not.\n",
        "We can assume that the cosine between movie and film is about 1. Lets's check it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WK406z-Ao4g"
      },
      "source": [
        "for w in word_to_idx:\n",
        "  if w == 'movie':\n",
        "    print(f\"index for movie : {word_to_idx[w]}\")\n",
        "  if w == 'film':\n",
        "    print(f\"index for film : {word_to_idx[w]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQiXnFseLqWp"
      },
      "source": [
        "ax2 = sns.heatmap(similarity_matrix[0:10,120:130],linewidths=2,cmap=\"plasma\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cwUjCnlCwk0"
      },
      "source": [
        "Indeed, these two words are synonyms ! (look at the cell (7,2), the yellow one) \n",
        "Therefore, two words that don't have the same context  at all (the cosine is 0) don't mean that they are antonyms.\n",
        "\n",
        "We can try we 2 random words that have a cosine near 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXPjDy9XgQJi"
      },
      "source": [
        "print(f\" word 163 : {idx_to_word[9]}\")\n",
        "print(f\" word 249 : {idx_to_word[128]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAuQxcH9gYXn"
      },
      "source": [
        "Indeed, the cosine of these two words is about 0 but they are not antonyms.  \n",
        "Let's find two antonyms, like man and woman"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-cjLcB5gz10"
      },
      "source": [
        "for w in word_to_idx:\n",
        "  if w =='man':\n",
        "    print(f\" index of the word man : {word_to_idx[w]}\")\n",
        "  if w =='woman':\n",
        "    print(f\" index of the word woman : {word_to_idx[w]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qK98u8uhNDq"
      },
      "source": [
        "And plot the heatmap:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdHWscE2hRbJ"
      },
      "source": [
        "ax8 = sns.heatmap(similarity_matrix[780:790,1015:1025],linewidths=2,cmap=\"plasma\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbK8JF9djGIY"
      },
      "source": [
        "We see that these 2 words don't have a 0 cosine (look at the cell (8,5) = 0.30), it's because they tend to appears within the same context. We should try to build a co_occurrence_matrix with a larger window, and we should see the cosine goes lower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN8LfJ_ngOkj"
      },
      "source": [
        "We will try to compute the cosine matrix in lower dimension to see if we can get more information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO2iLDB88bFi"
      },
      "source": [
        "cosine_matrix = cosine_similarity(reduced_tSNE,reduced_tSNE,transpose_q=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fvYSRvx8r6n"
      },
      "source": [
        "ax = sns.heatmap(cosine_matrix[:30,:30],linewidths=2,cmap=\"plasma\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmWGi2pdEzzt"
      },
      "source": [
        "* 2 words very close to each other have a cosine of 1.\n",
        "* 2 words very far from each other have a cosine of 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWtkRuBG2L0A"
      },
      "source": [
        "We can see from the above heatmap that lots of words seem very similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu38-5ZmFvMg"
      },
      "source": [
        "** **HOW DO YOU EXPLAIN THE RESULTS OBTAINED IN YOUR EXPERIMENTS? DISCUSS HERE** **\n",
        "\n",
        "If the cosine similarity is quite inaccurate in lower dimension (because we see that a lot of pairs of words have a cosine of 1) it's maybe because our window is too tight. If the window length would be higher, we could capture more semantic information. Futhermore, because of the space reduction, we lose some kind of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM5e-2kXF4XI"
      },
      "source": [
        "#### **[Let's play!] Analogies**\n",
        "\n",
        "Another useful property to check is analogy resolution via word vectors. In particular, we might want to check if analogies such \"man : king == woman : x\" bring results like \"x = queen\".\n",
        "\n",
        "In order to do so, we first need to define a ranking function that returns the top $K$ most similar words of a given word vector. We might not want to be too much restrctive and play with $K \\ge 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnoQdP-lGkzK"
      },
      "source": [
        "def get_top_K_indexes(data, K):\n",
        "    \"\"\"\n",
        "    Returns the top K indexes of a 1-dimensional array (descending order)\n",
        "    Example:\n",
        "        data = [0, 7, 2, 1]\n",
        "        best_indexes:\n",
        "        K = 1 -> [1] (data[1] = 7)\n",
        "        K = 2 -> [1, 2]\n",
        "        K = 3 -> [1, 2, 3]\n",
        "        K = 4 -> [1, 2, 3, 4]\n",
        "\n",
        "    :param data: 1-d dimensional array\n",
        "    :param K: number of highest value elements to consider\n",
        "\n",
        "    :return\n",
        "        - array of indexes corresponding to elements of highest value\n",
        "    \"\"\"\n",
        "    best_indexes = np.argsort(data, axis=0)[::-1]\n",
        "    best_indexes = best_indexes[:K]\n",
        "\n",
        "    return best_indexes\n",
        "\n",
        "def get_top_K_word_ranking(embedding_matrix, idx_to_word, word_to_idx,\n",
        "                           positive_listing, negative_listing, K):\n",
        "    \"\"\"\n",
        "    Finds the top K most similar words following this reasoning:\n",
        "        1. words that have highest similarity to words in positive_listing\n",
        "        2. words that have highest distance to words in negative_listing\n",
        "    \n",
        "    Positive and negative listing can be defined accordingly to a given analogy\n",
        "    Example:\n",
        "        \n",
        "        man : king :: woman : x\n",
        "    \n",
        "    positive_listing = ['king', 'woman']\n",
        "    negative_listing = ['man']\n",
        "\n",
        "    This is equivalent to: compute king - man + woman, and then find the\n",
        "    most similar candidate.\n",
        "    \n",
        "    :param embedding_matrix: embedding matrix of shape (words, embedding dimension).\n",
        "    Note that in the case of a co-occurrence matrix, the shape is (words, words).\n",
        "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param positive_listing: list of words that should have high similarity with\n",
        "                             top K retrieved ones.\n",
        "    :param negative_listing: list of words that should have high distance to\n",
        "                             top K retrieved ones.\n",
        "    :param K: number of best word matches to consider\n",
        "\n",
        "    :return\n",
        "        - top K word matches according to aforementioned criterium\n",
        "        - similarity values of top K word matches according to aforementioned\n",
        "          criterium\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Positive words (similarity)\n",
        "    positive_indexes = np.array([word_to_idx[word] for word in positive_listing])\n",
        "    word_positive_vector = np.sum(embedding_matrix[positive_indexes, :], axis=0)\n",
        "\n",
        "    # Negative words (distance)\n",
        "    negative_indexes = np.array([word_to_idx[word] for word in negative_listing])\n",
        "    word_negative_vector = np.sum(embedding_matrix[negative_indexes, :], axis=0)\n",
        "\n",
        "    # Find candidate words\n",
        "    target_vector = (word_positive_vector - word_negative_vector) / (len(positive_listing) + len(negative_listing))\n",
        "    total_indexes = np.concatenate((positive_indexes, negative_indexes))\n",
        "    valid_indexes = np.setdiff1d(np.arange(similarity_matrix.shape[0]), total_indexes)\n",
        "    candidate_vectors = embedding_matrix[valid_indexes]\n",
        "\n",
        "    candidate_similarities = cosine_similarity(candidate_vectors, target_vector, transpose_q=True)\n",
        "    candidate_similarities = candidate_similarities.ravel()\n",
        "\n",
        "    relative_indexes = get_top_K_indexes(candidate_similarities, K)\n",
        "    top_K_indexes = valid_indexes[relative_indexes]\n",
        "    top_K_words = [idx_to_word[idx] for idx in top_K_indexes]\n",
        "\n",
        "    return top_K_words, candidate_similarities[relative_indexes]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V4c-mOpNem7"
      },
      "source": [
        "Now do it yourself! Find some examples of analogies that hold and other that do not. Remember to give a proper explanation concerning obtained results.\n",
        "\n",
        "**Note**: 1-2 examples are sufficient. This exercise is just another way to inspect word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vva-_bylNxIE"
      },
      "source": [
        "### MODIFY THIS ###\n",
        "K = 15\n",
        "\n",
        "# Example analogy: tv : episodes :: film : x\n",
        "# positive listing -> [episodes, film]\n",
        "# negative listing ->  [tv]\n",
        "# masterpiece : superb :: x : tragic\n",
        "top_K_words, top_K_values = get_top_K_word_ranking(co_occurrence_matrix.todense(),\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['fear', 'frightened'],\n",
        "                                                   ['funny'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words)\n",
        "print('Top K values: ', top_K_values)\n",
        "\n",
        "top_K_words2, top_K_values2 = get_top_K_word_ranking(co_occurrence_matrix.todense(),\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['king', 'woman'],\n",
        "                                                   ['man'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words2)\n",
        "print('Top K values: ', top_K_values2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPG8PdSLNBc-"
      },
      "source": [
        "For the first example, I want to see words that are close to `fear` and `frightened` and very far from `funny`.  \n",
        "I was expecting something like `horror` or `zombies` but I got interresting results:  \n",
        "\n",
        "* words like `heroin`, `obedience` or `repressive` are kinda part of the lexicon but the others words don't really mean anything.\n",
        "\n",
        "For the second example, I tried the famous `king` - `man` + `woman` but that give inconsistants results\n",
        "\n",
        "But anyway, we can see that it's not very accurate. We could build a search engine with this method, but we need to improve it.  \n",
        "Maybe with a larger window context, and a larger vocabulary/dataset we could caught better results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxGPC1CC7mYn"
      },
      "source": [
        "#### **[Let's play!] Bias**\n",
        "\n",
        "When we talk about societies we are usually aware of their related biases (gender, race, sexual orientation, etc..). Indeed, this fact is reflected at textual level. For example, when we consider the word 'doctor' we usually think of a 'man', whereas when we consider the word 'nurse' we usually think of a 'woman'.\n",
        "\n",
        "Let's see if word embeddings reflect such harmful biases. Find an example of bias by following the analogy approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkuKPZsJUPgH"
      },
      "source": [
        "top_K_words3, top_K_values3 = get_top_K_word_ranking(co_occurrence_matrix.todense(),\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['doctor', 'mother'],\n",
        "                                                   ['father'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words3)\n",
        "print('Top K values: ', top_K_values3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twScPzVqUSmt"
      },
      "source": [
        "** **HOW DO YOU EXPLAIN THE RESULTS OBTAINED IN YOUR EXPERIMENTS? DISCUSS HERE** **\n",
        "\n",
        "There isn't a real biais because our model doesn't catch real semantics properties, it is very soft."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6cyDMwtLWQ4"
      },
      "source": [
        "## Better sparse word embeddings\n",
        "\n",
        "Until now we've played with the most basic type of word encoding, that is count-based co-occurrence matrix. However, there are better ways to encode words.\n",
        "\n",
        "In particular, we will explore positive pointwise mutual information (PPMI) weighting technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvsS4nY6LjOm"
      },
      "source": [
        "### PPMI\n",
        "\n",
        "Pointwise mutual information (PMI) is a weighting technique, just like tf-dif, that gives more weight to word pairs based on how often they occur within the same context window that we would have expected them to appear by chance.\n",
        "\n",
        "$PMI(w1, w2) = \\log_2 \\frac{P(w1, w2)}{P(w1)P(w2)}$\n",
        "\n",
        "PMI value range is $[-\\infty, \\infty]$, but negative values are a bit tricky, unless we have a very big corpus. Thus, it is more common to replace all negative PMI values with zero.\n",
        "\n",
        "$PPMI(w1, w2) = \\max(\\log_2 \\frac{P(w1, w2)}{P(w1)P(w2)}, 0)$\n",
        "\n",
        "Now, it's your turn to weight the count-based co-occurrence matrix with PPMI technique.\n",
        "\n",
        "**PPMI Memo**: \n",
        "Given a co-occurrence matrix C of shape (N, M), we can turn it into a PPMI matrix as follows:\n",
        "\n",
        "$p_{i,j} = \\frac{C_{i, j}}{\\sum_{i=1}^N \\sum_{j=1}^M C_{i,j}}$\n",
        "\n",
        "$p_{i,*} = \\frac{\\sum_{j=1}^M C_{i, j}}{\\sum_{i=1}^N \\sum_{j=1}^M C_{i,j}}$\n",
        "\n",
        "$p_{*,j} = \\frac{\\sum_{i=1}^N C_{i, j}}{\\sum_{i=1}^N \\sum_{j=1}^M C_{i,j}}$\n",
        "\n",
        "$PPMI_{i, j} = \\max(\\log_2 \\frac{p_{i, j}}{p_{i, *} \\, p_{*, j}}, 0)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DcQJ1dvmCY2",
        "outputId": "36628617-84c0-4547-e4ea-efe9bf20cb14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Function definition\n",
        "import math\n",
        "def convert_ppmi(co_occurrence_matrix):\n",
        "    \"\"\"\n",
        "    Converts a count-based co-occurrence matrix to a PPMI matrix\n",
        "\n",
        "    :param co_occurrence_matrix: count based co-occurrence matrix of shape (|V|, |V|)\n",
        "    \n",
        "    :return\n",
        "        - PPMI co-occurrence matrix of shape (|V|, |V|)\n",
        "    \"\"\"\n",
        "    if type(co_occurrence_matrix) == scipy.sparse.lil.lil_matrix:\n",
        "      co_occurrence_matrix = co_occurrence_matrix.todense()\n",
        "      co_occurrence_matrix = co_occurrence_matrix.A\n",
        "    v = len(co_occurrence_matrix)\n",
        "    sum_col = sum(co_occurrence_matrix)\n",
        "    d = sum(sum_col)\n",
        "    S = np.zeros((v,v))\n",
        "    for i in range(0,v):\n",
        "      row = co_occurrence_matrix[i]\n",
        "      for j in range(0,v):\n",
        "        pij = row[j] / d\n",
        "        if pij == 0:\n",
        "          S[i][j] = 0\n",
        "        else:\n",
        "          pi = sum(row) / d\n",
        "          pj = sum_col[j] / d\n",
        "          u = math.log2(pij/(pi*pj))\n",
        "          S[i][j] = max(u,0)\n",
        "    return S\n",
        "\n",
        "# Testing\n",
        "\n",
        "print(\"Computing PPMI co-occurrence matrix...\")\n",
        "ppmi_occurrence_matrix = convert_ppmi(co_occurrence_matrix)\n",
        "print(\"PPMI completed!\")\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def evaluate_ppmi_matrix(matrix):\n",
        "    is_sparse = False\n",
        "\n",
        "    if hasattr(scipy.sparse, type(matrix).__name__):\n",
        "        print(\"Detected sparse PPMI co-occurrence matrix!\")\n",
        "        is_sparse = True\n",
        "\n",
        "    # Check symmetry\n",
        "    print(\"[Co-occurrence PPMI matrix Evaluation] Symmetry checking...\")\n",
        "    if is_sparse:\n",
        "        try:\n",
        "            assert (matrix != matrix.transpose()).nnz == 0\n",
        "        except AssertionError:\n",
        "            assert sparse_allclose(matrix, matrix.transpose())\n",
        "    else:\n",
        "        try:\n",
        "            assert np.equal(matrix, matrix.transpose()).all()\n",
        "        except AssertionError:\n",
        "            assert np.allclose(matrix, matrix.transpose())\n",
        "\n",
        "    # A very simple example\n",
        "    print(\"[Co-occurrence PPMI matrix Evaluation] Toy example checking...\")\n",
        "\n",
        "    toy_df = pd.DataFrame.from_dict({\n",
        "        'sentence_1': [\"All that glitters is not gold\"],\n",
        "        'sentence_2': [\"All in all I like this assignment\"]\n",
        "    })\n",
        "\n",
        "    # We should already have download co-occurrence benchmark data\n",
        "    benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n",
        "    toy_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark')\n",
        "    toy_idx_to_word = np.load(os.path.join(toy_path, 'toy_idx_to_word.npy'), allow_pickle=True).item()\n",
        "    toy_word_to_idx = np.load(os.path.join(toy_path, 'toy_word_to_idx.npy'), allow_pickle=True).item()\n",
        "    toy_valid_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_count.npy'))\n",
        "\n",
        "    toy_ppmi_matrix = convert_ppmi(toy_valid_matrix)\n",
        "    toy_valid_ppmi_matrix = np.load(os.path.join(toy_path, 'toy_co_occurrence_matrix_ppmi.npy'))\n",
        "\n",
        "    if is_sparse:\n",
        "        try:\n",
        "            assert (toy_ppmi_matrix != toy_valid_ppmi_matrix).nnz == 0\n",
        "        except AssertionError:\n",
        "            assert sparse_allclose(toy_ppmi_matrix, toy_valid_ppmi_matrix)\n",
        "    else:\n",
        "        try:\n",
        "            assert np.equal(toy_ppmi_matrix, toy_valid_ppmi_matrix).all()\n",
        "        except AssertionError:\n",
        "            assert np.allclose(toy_ppmi_matrix, toy_valid_ppmi_matrix)\n",
        "\n",
        "\n",
        "print('Evaluating PPMi matrix conversion...')\n",
        "evaluate_ppmi_matrix(ppmi_occurrence_matrix)\n",
        "print('Evaluation completed!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing PPMI co-occurrence matrix...\n",
            "PPMI completed!\n",
            "Evaluating PPMi matrix conversion...\n",
            "[Co-occurrence PPMI matrix Evaluation] Symmetry checking...\n",
            "[Co-occurrence PPMI matrix Evaluation] Toy example checking...\n",
            "Evaluation completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQwP1ee1_NPV"
      },
      "source": [
        "### **Got Stuck?**\n",
        "\n",
        "If you are stuck, but still want to try out following sections, you can experiment with a valid PPMI co-occurrence matrix provided by us as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdRRGT6Z_QSE"
      },
      "source": [
        "# benchmark_path = os.path.join(os.getcwd(), 'Benchmark')\n",
        "# valid_data_benchmark_path = os.path.join(benchmark_path, 'co-occurrence_count_benchmark', \"{}.npy\")\n",
        "\n",
        "# ppmi_occurrence_matrix = np.load(valid_data_benchmark_path.format('valid_co-occurrence_matrix_ppmi'))\n",
        "\n",
        "# print('PPMI Co-occurrence matrix shape: ', ppmi_occurrence_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQOx7ZtLyJTn"
      },
      "source": [
        "### Visualization (cont'd)\n",
        "\n",
        "Let's see if these weighting techniques have brought some change at visualization level!\n",
        "\n",
        "Pick a dimensionality reduction technique and explore the new embedding space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa2E6yWDyb39"
      },
      "source": [
        "reduced_ppmi_tSNE = reduce_tSNE(ppmi_occurrence_matrix)\n",
        "visualize_embeddings(reduced_ppmi_tSNE, ['good', 'love', 'beautiful','enjoy','incredible','bad','awful','horrible','movie','film','ugly'], word_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrJ4CU7R67eX"
      },
      "source": [
        "That is a bit better than before ! \n",
        "We spot that `movie` and `film` are closer to `ugly` and `bad` than `beautiful`. \n",
        "With the PPMI matrix, we can clearly see that `beautiful` is closer to `enjoy` than `bad`, that something we couldn't clearly see a few steps before.  \n",
        "It seems that we have a better spacial representation of words.  \n",
        "Indeed, PPMI measures how often a pairing appears relative to either word appearing independently. In other words, it’s measuring the dependence between two words !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Ud--aA8Jlw"
      },
      "source": [
        "Feel free to play with visualization!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45jFTrIU5RXR"
      },
      "source": [
        "visualize_embeddings(reduced_ppmi_tSNE,['man','queen','woman','king'],word_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpFmbxWn-Ris"
      },
      "source": [
        "There is still a problem `man` seems closer to `queen` than `king` but is's better than before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY70qU-53rFy"
      },
      "source": [
        "### [Let's play!] Embedding properties (cont'd)\n",
        "\n",
        "Choose a word embedding property to analyse (synonyms, analogies, bias) and either select and describe a new example or pick an old one and compare previously achieved results with current ones!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGvEcMrs4Yvq"
      },
      "source": [
        "K=30\n",
        "top_K_words3, top_K_values3 = get_top_K_word_ranking(ppmi_occurrence_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['fear', 'frightened'],\n",
        "                                                   ['funny'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words3)\n",
        "print('Top K values: ', top_K_values3)\n",
        "\n",
        "top_K_words4, top_K_values4 = get_top_K_word_ranking(ppmi_occurrence_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['king', 'woman'],\n",
        "                                                   ['man'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words4)\n",
        "print('Top K values: ', top_K_values4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4d24Ft_kldG"
      },
      "source": [
        "I took the same examples that before but we don't have real improvment, we need to improve more our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc4YevLsx3xY"
      },
      "source": [
        "# [Part II] Dense embeddings\n",
        "\n",
        "Until now we've worked with sparse embedding methods, which lead to high dimensional word embeddings (dimension equal to |V|). The main drawback of such approach is that words belong to separate dimensions. Thus, in order to check if two words have similar contexts we need to have a large corpus available.\n",
        "\n",
        "To this end, we might prefer a dense embedding technique, such that all words are encoded to high dimensional space, much smaller than |V| (generally up to $\\sim$ 1000). A dense representation is also convenient from a machine learning point of view: we have fewer parameters to learn and, thus, models are less prone to overfitting. Moreover, words do not belong to separate dimensions anymore and semantic relationships are easily modelled.\n",
        "\n",
        "In this section, we will experiment with pre-trained dense embedding models and compare them to previously described sparse methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i_gxgZK5VGM"
      },
      "source": [
        "## Working with a pre-trained model\n",
        "\n",
        "The first step consists in choosing and downloading a pre-trained embedding model. For the purpose of this assignment, we limit to classic models, such as Word2Vec and GloVe.\n",
        "\n",
        "Furthermore, some pre-trained embedding model versions may be quite resource demanding, depending on the embedding dimension and on the vocabulary size. We recommend sticking to low dimensional spaces (50, 100, 200) to avoid being stuck waiting for too much time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoU1fqAs5XxI"
      },
      "source": [
        "### Download embedding model\n",
        "\n",
        "Downloading a pre-trained embedding model is quite simple to due existing ad hoc wrappers. In particular, we will use [Gensim](https://radimrehurek.com/gensim/) library for both embedding models as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV0RQIFT-Sd3"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "def load_embedding_model(model_type, embedding_dimension=50):\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "\n",
        "    download_path = \"\"\n",
        "\n",
        "    # Find the correct embedding model name\n",
        "    if model_type.strip().lower() == 'word2vec':\n",
        "        download_path = \"word2vec-google-news-300\"\n",
        "\n",
        "    elif model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove\")\n",
        "\n",
        "    # Check download\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Word2Vec: 300\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model\n",
        "\n",
        "\n",
        "# Modify these variables as you wish!\n",
        "# Glove -> 50, 100, 200, 300\n",
        "# Word2Vec -> 300\n",
        "embedding_model_type = \"glove\"\n",
        "embedding_dimension = 50\n",
        "\n",
        "embedding_model = load_embedding_model(embedding_model_type, embedding_dimension)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEL3Hrjl5dfs"
      },
      "source": [
        "### Out of vocabulary (OOV) words\n",
        "\n",
        "Before evaluating pre-trained dense word embeddings, it is good practice to check if the model is consistent with our dataset. To do so, we check the number of out-of-vocabulary (OOV) terms.\n",
        "\n",
        "If the OOV amount is negligible, we can just keep going. On the other hand, we might want to handle OOV terms by assigning them a specific word vector.\n",
        "\n",
        "**Which one?** One common practice is to assign a random vector, since the embedding model will be part of a deep learning model and, thus, word vectors might be trained during the learning process. Even if that is the case, we can assign an embedding that is more meaningful rather than a random one: for example, we can identify the word embedding of an OOV term as the mean of its neighbour word embeddings.\n",
        "\n",
        "Check out OOV terms and assign a meaningful word embedding. Then, at the visualization step, check if this strategy reflects words semantic properties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_L_ewVPH7VQ",
        "outputId": "d094031a-d0d1-4c9d-80c9-871347a3d9bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Function definition\n",
        "\n",
        "def check_OOV_terms(embedding_model, word_listing):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    oov = []\n",
        "    for w in word_listing:\n",
        "      if w not in embedding_model.vocab:\n",
        "        oov.append(w)\n",
        "    return oov\n",
        "\n",
        "\n",
        "oov_terms = check_OOV_terms(embedding_model, word_listing)\n",
        "\n",
        "print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_terms), float(len(oov_terms)) / len(word_listing)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total OOV terms: 1321 (0.10%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqXbsRjymBWt"
      },
      "source": [
        "oov_terms[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp6xoekEwliG"
      },
      "source": [
        "These oov words are mostly mistyped words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjNoxZb--5O-"
      },
      "source": [
        "### Handling OOV words\n",
        "\n",
        "Now we proceed on building the embedding matrix, while handling OOV terms at the same time. \n",
        "\n",
        "Experiment with/without the OOV custom encoding strategy.\n",
        "\n",
        "**NOTE**: Here we ask you to implement both OOV strategies! Feel free to either write two separate functions or modify the given function signature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC1F_44lJUDF"
      },
      "source": [
        "# Function definition\n",
        "def build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, idx_to_word, oov_terms, co_occurrence_count_matrix,random=False):\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "    :param co_occurruence_count_matrix: the co-occurrence count matrix of the given dataset (window size 1)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    if type(co_occurrence_count_matrix) == scipy.sparse.lil.lil_matrix:\n",
        "      co_occurrence_count_matrix = co_occurrence_count_matrix.A\n",
        "\n",
        "    v = len(word_to_idx)\n",
        "    embedding_matrix = np.zeros((v,embedding_dimension))\n",
        "    i=0\n",
        "    for w in word_to_idx:\n",
        "      if w in embedding_model.vocab:\n",
        "        embedding_matrix[i] = embedding_model.get_vector(w)\n",
        "      else:\n",
        "        if random:\n",
        "          embedding_matrix[i] = np.random.rand(50,)\n",
        "        else:\n",
        "          \"\"\"\n",
        "        - take the sparse vector representation of the oov\n",
        "        - retrieve all the words which appear in the context of the oov\n",
        "        - take the dense representation of these words, if a word doesn't have one we don't care and we go to the next word\n",
        "        - make an average vector\n",
        "        - this is the dense representation of the oov ! \n",
        "        \"\"\"\n",
        "          index = word_to_idx[w]\n",
        "          wrapper = np.zeros((0,embedding_dimension))\n",
        "          sparse_vector = co_occurrence_count_matrix[index]\n",
        "          for j,c in enumerate(sparse_vector):\n",
        "            if c != 0:\n",
        "              word = idx_to_word[j]\n",
        "              if word in embedding_model.vocab:\n",
        "                dense_vector = embedding_model.get_vector(word)\n",
        "                wrapper = np.vstack((wrapper,dense_vector))\n",
        "          if wrapper.shape[0] >1:\n",
        "            wrapper = np.delete(wrapper,0,0)\n",
        "            dense_oov = np.mean(wrapper,axis=0)\n",
        "          else:\n",
        "            dense_oov = np.random.rand(50,)\n",
        "          embedding_matrix[i] = dense_oov\n",
        "      i+=1\n",
        "\n",
        "    return embedding_matrix\n",
        "# Testing\n",
        "co_occurrence_count_matrix = co_occurrence_count(df,idx_to_word,word_to_idx,1)\n",
        "embedding_matrix_r = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, idx_to_word, oov_terms,co_occurrence_matrix,True)\n",
        "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, idx_to_word , oov_terms,co_occurrence_matrix)\n",
        "print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hirKvw8x5kd4"
      },
      "source": [
        "## Embedding visualization (cont'd)\n",
        "\n",
        "We are now ready to visualize pre-trained word embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwXpnnAalbfE"
      },
      "source": [
        "## Non Random OOV "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccE5hxlARz4X"
      },
      "source": [
        "r = reduce_tSNE(embedding_matrix)\n",
        "visualize_embeddings(r,['good', 'love', 'beautiful','enjoy','incredible','bad','awful','horrible','movie','film','ugly'], word_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-3nGZVenAkq"
      },
      "source": [
        "visualize_embeddings(r,['man','woman','queen','king'], word_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiTkkoEAnFLz"
      },
      "source": [
        "It works as expected, `man` and `woman` are close to each other and `queen` and `king` too"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNuyPX4kmnks"
      },
      "source": [
        "## Random OOV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0IsQvqRjWwe"
      },
      "source": [
        "er = reduce_tSNE(embedding_matrix_r)\n",
        "visualize_embeddings(er,['good', 'love', 'beautiful','enjoy','incredible','bad','awful','horrible','movie','film','ugly'], word_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COdV-UOjkkEt"
      },
      "source": [
        "visualize_embeddings(er,['man','woman','queen','king'], word_to_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnSr_tDqnQoP"
      },
      "source": [
        "There is no great discrepancies between generating embeded oov words randomly or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxVC_fOo5oqQ"
      },
      "source": [
        "## [Let's play!] Embedding properties (cont'd)\n",
        "\n",
        "Choose a word embedding property to analyse (synonyms, analogies, bias) and either select and describe a new example or pick an old one and compare previously achieved results with current ones!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82Fjuqg9SJIO"
      },
      "source": [
        "K=30\n",
        "top_K_words5, top_K_values5 = get_top_K_word_ranking(embedding_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['fear', 'frightened'],\n",
        "                                                   ['funny'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words5)\n",
        "print('Top K values: ', top_K_values5)\n",
        "\n",
        "top_K_words6, top_K_values6 = get_top_K_word_ranking(embedding_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['king', 'woman'],\n",
        "                                                   ['man'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words6)\n",
        "print('Top K values: ', top_K_values6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUaCWdZg-_ZL"
      },
      "source": [
        "This time we have really good analogies ! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32s8qOCvDNKi"
      },
      "source": [
        "top_K_words7, top_K_values7 = get_top_K_word_ranking(embedding_matrix,\n",
        "                                                   idx_to_word,\n",
        "                                                   word_to_idx,\n",
        "                                                   ['doctor', 'mother'],\n",
        "                                                   ['father'],\n",
        "                                                   K)\n",
        "print('Top K words: ', top_K_words7)\n",
        "print('Top K values: ', top_K_values7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVAyFU2wDOzt"
      },
      "source": [
        "Therefore, the bias is stronger than before :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyPDNB56UhmM"
      },
      "source": [
        "# Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "Don't forget that your feedback is very important! Your suggestions help us improving course material."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNXyqKePNbth"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Is it ok if I work with a small slice of the dataset?**\n",
        "\n",
        "**A:** Yes, it is perfectly ok! The aim of this assignment is to look at word embedding methods and assess semantic properties. Large datasets usually imply large vocabularies and efficient sparse encoding methods have to be considered. Since such methods (see scipy documentation) might be complex to handle (especially under a colab session), you are free to work with small corpora.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Do I have to use both dimensionality reduction methods?**\n",
        "\n",
        "**A:** Just one is fine! We suggest to try both of them at least once!\n",
        "\n",
        "---\n",
        "\n",
        "**Q: I'm struggling find good examples for analogies, bias and other scenarios!**\n",
        "\n",
        "**A:** It is perfectly fine, this is just a simple example where we want you to look at\n",
        "different encoding methods. Try to find at least one example that sounds good to you and motivate obtained results. Most probably, you won't find a perfect corrispondence of your expectations, but for us what's important is that you develop a critic approach, baring in mind that data has to explored before anything else.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Isn't stopwords removal excessive?**\n",
        "\n",
        "**A:** Indeed, removing all stopwords might alter sentence meaning! However, they also alter co-occurrence matrices due to their high frequency. Not ignoring them leads to poor results when considering semantic properties. It's up to you whether removing or keeping stopwords! (remember to comment the corresponding method under [Some Cleaning](https://colab.research.google.com/drive/1UkGz0vdhPXh9NeApG7mYtY6e-jRcR3if#scrollTo=2TLTu0-2JQwi&line=3&uniqifier=1) section.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Can we modify functions signature?**\n",
        "\n",
        "**A:** Functions that you have to complete can be modified as you please! Current functions definition should consider all required inputs in most cases.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Can we modify the dataset slicing step**\n",
        "\n",
        "**A:** Yes, of course! The current slicing is just one possibility.\n",
        "\n",
        "---\n"
      ]
    }
  ]
}