{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bidaf_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1q6JJXsQvZONqiS4NEslvgYcg-tXfMUcf",
      "authorship_tag": "ABX9TyNCSoSBL2SPO5PCNAwlD7ej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CorentinMAG/NLP/blob/main/BIDAF/bidaf_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrXpyz2vZhs1"
      },
      "source": [
        "In order to run this notebook, first things you should do are :\r\n",
        "* mount your drive endpoint\r\n",
        "* go at the end and modify paths ( i don't know if numpy.save() also create missing folders )\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hETK3Axg43yB",
        "outputId": "5655f589-ba9b-446d-b70c-87ffb35bfe05"
      },
      "source": [
        "import json\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import nltk\r\n",
        "from nltk import word_tokenize\r\n",
        "nltk.download('punkt')\r\n",
        "import gensim.downloader as gloader\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import re\r\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqLS5R574tdD"
      },
      "source": [
        "EMBEDDING_SIZE = 300\r\n",
        "\r\n",
        "def load_dataset(path, record_path = ['data', 'paragraphs', 'qas', 'answers'], verbose = True):\r\n",
        "\r\n",
        "  if verbose:\r\n",
        "      print(\"Reading the json file\")\r\n",
        "  # if the file encoding is not UTF8 an exception should be raised    \r\n",
        "  file = json.loads(open(path).read())\r\n",
        "\r\n",
        "  if verbose:\r\n",
        "      print(\"[INFO] processing...\")\r\n",
        "\r\n",
        "  # parsing different level's in the json file\r\n",
        "  js = pd.json_normalize(file , record_path )\r\n",
        "  m = pd.json_normalize(file, record_path[:-1] )\r\n",
        "  r = pd.json_normalize(file,record_path[:-2])\r\n",
        "  t = pd.json_normalize(file,record_path[0])\r\n",
        "\r\n",
        "  #combining it into single dataframe\r\n",
        "  idx = np.repeat(r['context'].values, r.qas.str.len())\r\n",
        "  ndx  = np.repeat(m['id'].values,m['answers'].str.len())\r\n",
        "  m['context'] = idx\r\n",
        "  js['q_idx'] = ndx\r\n",
        "  main = pd.concat([ m[['id','question','context']].set_index('id'), js.set_index('q_idx')],1,sort = False).reset_index()\r\n",
        "  main['c_id'] = main['context'].factorize()[0]\r\n",
        "  if verbose:\r\n",
        "      print(f\"[INFO] there are {main.shape[0]} questions with single answer\")\r\n",
        "      print(f\"[INFO] there are {main.groupby('c_id').sum().shape[0]} different contexts\")\r\n",
        "      print(f\"[INFO] there are {len(t)} unrelated subjects\")\r\n",
        "      print(\"[INFO] Done\")\r\n",
        "  return main\r\n",
        "\r\n",
        "def download_glove_model(embedding_dimension = 50):\r\n",
        "  download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\r\n",
        "  try:\r\n",
        "    print('[INFO] downloading glove {}'.format(embedding_dimension))\r\n",
        "    emb_model = gloader.load(download_path)\r\n",
        "    print('[INFO] done !')\r\n",
        "  except ValueError as e:\r\n",
        "      print(\"Glove: 50, 100, 200, 300\")\r\n",
        "      raise e\r\n",
        "  return emb_model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2virMOF85A8W",
        "outputId": "ef3172ee-7cbe-4f64-f2ed-36a4b791b7fa"
      },
      "source": [
        "dataset_path = os.path.join(os.getcwd(),'drive/MyDrive/NLP/SQUAD_project/data/training_set.json')\r\n",
        "squad_dataset = load_dataset(dataset_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the json file\n",
            "[INFO] processing...\n",
            "[INFO] there are 87599 questions with single answer\n",
            "[INFO] there are 18891 different contexts\n",
            "[INFO] there are 442 unrelated subjects\n",
            "[INFO] Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lzHPUWA45dzO",
        "outputId": "b622f77d-4890-4a61-8891-0bde3723e094"
      },
      "source": [
        "squad_dataset.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>515</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>188</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>279</td>\n",
              "      <td>the Main Building</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>381</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>92</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ... c_id\n",
              "0  5733be284776f41900661182  ...    0\n",
              "1  5733be284776f4190066117f  ...    0\n",
              "2  5733be284776f41900661180  ...    0\n",
              "3  5733be284776f41900661181  ...    0\n",
              "4  5733be284776f4190066117e  ...    0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCWKKV2RAhML"
      },
      "source": [
        "SAMPLES = squad_dataset.shape[0]\r\n",
        "\r\n",
        "def preprocess_sentence(text):\r\n",
        "  text = text.lower()\r\n",
        "  #text = re.sub(r'(\\.)',r' \\1 ', text)\r\n",
        "  text = text.strip()\r\n",
        "  return text\r\n",
        "\r\n",
        "def clean_dataset(dataset):\r\n",
        "\r\n",
        "  _dataset = dataset.copy()\r\n",
        "\r\n",
        "  cleaned_questions = _dataset['question'].apply(preprocess_sentence)\r\n",
        "  cleaned_texts = _dataset['text'].apply(preprocess_sentence)\r\n",
        "\r\n",
        "  unique_context = pd.Series(_dataset['context'].unique())\r\n",
        "  count_c = _dataset.groupby('c_id').count()['text']\r\n",
        "  cleaned_contexts = unique_context.apply(preprocess_sentence)\r\n",
        "\r\n",
        "  _dataset['question'] = cleaned_questions\r\n",
        "  _dataset['text'] = cleaned_texts\r\n",
        "  _dataset['context'] = pd.Series(np.repeat(cleaned_contexts, count_c).tolist())\r\n",
        "\r\n",
        "  return _dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFxiBU06B5vA"
      },
      "source": [
        "squad = clean_dataset(squad_dataset)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq8B58osDLvX"
      },
      "source": [
        "def get_tokenizer(dataset, glove_model = None):\r\n",
        "\r\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\r\n",
        "  char_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = True, filters = '', oov_token = 'UNK', num_words = 200) # we will only keep the 200 - 1 most frequents characters (otherwise oom issue)\r\n",
        "                                                                                                                              # others tokens are replaced by UNK token \r\n",
        "                                                                                                                              # we keep 1 - 199 and 1 is UNK token (so we keep 198 tokens)\r\n",
        "  if glove_model == None:\r\n",
        "    glove_model = download_glove_model(EMBEDDING_SIZE)\r\n",
        "\r\n",
        "  tokenized_questions = dataset['question'].apply(word_tokenize).to_list()\r\n",
        "\r\n",
        "  contexts = pd.Series(dataset['context'].unique())\r\n",
        "  count = dataset.groupby('c_id').count()['context']\r\n",
        "  tokenized_contexts = contexts.apply(word_tokenize).to_list()\r\n",
        "\r\n",
        "  sequences = glove_model.index2entity + tokenized_questions + tokenized_contexts\r\n",
        "\r\n",
        "  del glove_model # we  don't need anymore the glove model\r\n",
        "\r\n",
        "  tokenizer.fit_on_texts(sequences)\r\n",
        "  char_tokenizer.fit_on_texts(dataset['question'] + dataset['context'])\r\n",
        "\r\n",
        "  return tokenizer, char_tokenizer\r\n",
        "\r\n",
        "\r\n",
        "def update_tokenizer(dataset, tokenizer, char_tokenizer):\r\n",
        "\r\n",
        "  tokenized_questions = dataset['question'].apply(word_tokenize).to_list()\r\n",
        "\r\n",
        "  contexts = pd.Series(dataset['context'].unique())\r\n",
        "  count = dataset.groupby('context').count()['c_id'].reset_index(drop = True)\r\n",
        "  tokenized_contexts = contexts.apply(word_tokenize).to_list()\r\n",
        "\r\n",
        "  sequences = tokenized_questions + tokenized_contexts\r\n",
        "  tokenizer.fit_on_texts(sequences)\r\n",
        "\r\n",
        "  char_tokenizer.fit_on_texts(dataset['question'] + dataset['context'])\r\n",
        "\r\n",
        "\r\n",
        "def get_start_end(row):\r\n",
        "\r\n",
        "  context = row['context']\r\n",
        "  answer = row['text']\r\n",
        "  tok_answer = word_tokenize(answer)\r\n",
        "\r\n",
        "  _start = context.find(answer)\r\n",
        "\r\n",
        "  lc = context[:_start]\r\n",
        "  lc = word_tokenize(lc)\r\n",
        "\r\n",
        "  start = len(lc)\r\n",
        "  end = start + len(tok_answer)\r\n",
        "\r\n",
        "  row['start'] = start\r\n",
        "  row['end'] = end\r\n",
        "\r\n",
        "  return row\r\n",
        "\r\n",
        "def tokenize(dataset, tokenizer, char_tokenizer):\r\n",
        "\r\n",
        "  _dataset = dataset.copy()\r\n",
        "\r\n",
        "  tokenized_questions = _dataset['question'].apply(word_tokenize).to_list()\r\n",
        "  tokenized_contexts = _dataset['context'].apply(word_tokenize).to_list()\r\n",
        "\r\n",
        "  t_q = tokenizer.texts_to_sequences(tokenized_questions)\r\n",
        "  t_c = tokenizer.texts_to_sequences(tokenized_contexts)\r\n",
        "\r\n",
        "  c_q = []\r\n",
        "  c_c = []\r\n",
        "\r\n",
        "  for question, context in zip(tokenized_questions, tokenized_contexts):\r\n",
        "    _q = char_tokenizer.texts_to_sequences(question)\r\n",
        "    _c = char_tokenizer.texts_to_sequences(context)\r\n",
        "    c_q.append(_q)\r\n",
        "    c_c.append(_c)\r\n",
        "\r\n",
        "  _dataset['tokenized_question'] = t_q\r\n",
        "  _dataset['tokenized_context'] = t_c\r\n",
        "\r\n",
        "  _dataset['char_tokenized_question'] = pd.Series(c_q)\r\n",
        "  _dataset['char_tokenized_context'] = pd.Series(c_c)\r\n",
        "\r\n",
        "  return _dataset\r\n",
        "\r\n",
        "def split(dataset, test_size = 0.2, random_state = 42):\r\n",
        "\r\n",
        "  # random_state for deterministic state\r\n",
        "\r\n",
        "  tr, vl = train_test_split(dataset, test_size = test_size, random_state = random_state)\r\n",
        "  tr.reset_index(drop = True, inplace = True)\r\n",
        "  vl.reset_index(drop = True, inplace = True)\r\n",
        "\r\n",
        "  return tr,vl\r\n",
        "\r\n",
        "def convert(context , coord = None, tokenizer = None):\r\n",
        "  if coord:\r\n",
        "    start = coord[0]\r\n",
        "    end = coord[1]\r\n",
        "    if type(context) == str:\r\n",
        "      context = word_tokenize(context)\r\n",
        "      answer = context[start:end]\r\n",
        "      return ' '.join(answer).strip()\r\n",
        "    else:\r\n",
        "      answer = ''\r\n",
        "      for i in range(start, end):\r\n",
        "        t = context[i]\r\n",
        "        answer+= tokenizer.index_word[t] + ' '\r\n",
        "      return answer.strip()\r\n",
        "  else:\r\n",
        "    if type(context) == str:\r\n",
        "      return context\r\n",
        "    else:\r\n",
        "      c = ''\r\n",
        "      for t in context:\r\n",
        "        c += tokenizer.index_word[t] + ' '\r\n",
        "      return c.strip()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8pXuOXlpLsx"
      },
      "source": [
        "tr_df, vl_df = split(squad)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGk_DpS3RS32",
        "outputId": "1b71aaf8-76c4-4b11-cb63-be37b15d5090"
      },
      "source": [
        "tr_df.shape[0],vl_df.shape[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70079, 17520)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0VbHyp4YSUx"
      },
      "source": [
        "Our vocabulary is based on the Glove vocabulary, and we add terms from the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hY8Z7HQFKjo",
        "outputId": "79c29ed3-668d-44cc-9dc7-5efce7ae8703"
      },
      "source": [
        "tokenizer, char_tokenizer = get_tokenizer(tr_df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] downloading glove 300\n",
            "[=================================================-] 98.7% 371.3/376.1MB downloaded[INFO] done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILsFqw2bKf5L",
        "outputId": "8406eab0-d15e-4d35-de66-6f9d6ca705ee"
      },
      "source": [
        "print(len(tokenizer.word_index))\r\n",
        "len(char_tokenizer.word_index)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "429063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1263"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWoZPgxdYbkz"
      },
      "source": [
        "We then update our vocabulary with terms from the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzv_ZS3Zqifn"
      },
      "source": [
        "update_tokenizer(vl_df, tokenizer, char_tokenizer)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLwNWJsorIae",
        "outputId": "33e62c56-2e20-4b44-97ab-28881b4c143d"
      },
      "source": [
        "print(len(tokenizer.word_index))\r\n",
        "len(char_tokenizer.word_index)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "429757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1265"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM8oYX_SHkni"
      },
      "source": [
        "# take a while\r\n",
        "\r\n",
        "tr_df = tr_df.apply(get_start_end, axis = 1)\r\n",
        "vl_df = vl_df.apply(get_start_end, axis = 1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox1J7Z8YhVcv"
      },
      "source": [
        "we get rid of samples where the answer doesn't match the context (maybe there is a typo in the answer or the context).  \r\n",
        "To avoid to discard many samples, we could lemmatize / stem the text.   \r\n",
        "Obviously, lemmatization is a better choice for our task, but if we want a really accurate lemmatization processing, we need to do POS tagging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eTsENy6jU-t",
        "outputId": "7502bcae-a45f-4845-92fd-69ce5e4c61a2"
      },
      "source": [
        "tr_df[tr_df['start'] == -1].shape[0], vl_df[vl_df['start'] == -1].shape[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_KJeyPxVZG6"
      },
      "source": [
        "tr_df = tokenize(tr_df, tokenizer, char_tokenizer)\r\n",
        "vl_df = tokenize(vl_df, tokenizer, char_tokenizer)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "VJd7p05zjg0e",
        "outputId": "afaa908f-0646-4516-bf25-24ce3affd052"
      },
      "source": [
        "tr_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>tokenized_question</th>\n",
              "      <th>tokenized_context</th>\n",
              "      <th>char_tokenized_question</th>\n",
              "      <th>char_tokenized_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>572667e6708984140094c4f9</td>\n",
              "      <td>what team had dallas green managed in 1980?</td>\n",
              "      <td>after over a dozen more subpar seasons, in 198...</td>\n",
              "      <td>154</td>\n",
              "      <td>phillies</td>\n",
              "      <td>8880</td>\n",
              "      <td>29</td>\n",
              "      <td>30</td>\n",
              "      <td>[10, 308, 48, 11807, 645, 2131, 5, 2626, 8]</td>\n",
              "      <td>[60, 82, 9, 6736, 61, 70019, 1739, 2, 5, 3371,...</td>\n",
              "      <td>[[20, 11, 5, 4], [4, 3, 5, 16], [11, 5, 13], [...</td>\n",
              "      <td>[[5, 17, 4, 3, 10], [8, 24, 3, 10], [5], [13, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56dec2483277331400b4d712</td>\n",
              "      <td>which candidate withdrew from the presidential...</td>\n",
              "      <td>schwarzenegger's endorsement in the republican...</td>\n",
              "      <td>156</td>\n",
              "      <td>rudy giuliani</td>\n",
              "      <td>2311</td>\n",
              "      <td>23</td>\n",
              "      <td>25</td>\n",
              "      <td>[26, 2788, 4160, 22, 1, 1533, 697, 5, 416, 3, ...</td>\n",
              "      <td>[1083, 18, 9105, 5, 1, 1466, 476, 3, 1, 419, 1...</td>\n",
              "      <td>[[20, 11, 6, 14, 11], [14, 5, 7, 13, 6, 13, 5,...</td>\n",
              "      <td>[[9, 14, 11, 20, 5, 10, 39, 3, 7, 3, 19, 19, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5726e5995951b619008f81bb</td>\n",
              "      <td>captive animals can distinguish co-inhabitats ...</td>\n",
              "      <td>it has been observed that well-fed predator an...</td>\n",
              "      <td>224</td>\n",
              "      <td>wild ones outside the area</td>\n",
              "      <td>9822</td>\n",
              "      <td>38</td>\n",
              "      <td>43</td>\n",
              "      <td>[11887, 726, 64, 3732, 419168, 22, 10, 47, 135...</td>\n",
              "      <td>[29, 39, 58, 2315, 19, 63224, 4420, 726, 5, 9,...</td>\n",
              "      <td>[[14, 5, 18, 4, 6, 24, 3], [5, 7, 6, 16, 5, 12...</td>\n",
              "      <td>[[6, 4], [11, 5, 9], [22, 3, 3, 7], [8, 22, 9,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5726486f708984140094c157</td>\n",
              "      <td>the results of which battle allowed the britis...</td>\n",
              "      <td>after returning from egypt, napoleon engineere...</td>\n",
              "      <td>919</td>\n",
              "      <td>the battle of trafalgar</td>\n",
              "      <td>8418</td>\n",
              "      <td>158</td>\n",
              "      <td>162</td>\n",
              "      <td>[1, 1323, 3, 26, 325, 494, 1, 131, 7, 6279, 15...</td>\n",
              "      <td>[60, 3985, 22, 597, 2, 544, 9788, 9, 2312, 5, ...</td>\n",
              "      <td>[[4, 11, 3], [10, 3, 9, 15, 12, 4, 9], [8, 17]...</td>\n",
              "      <td>[[5, 17, 4, 3, 10], [10, 3, 4, 15, 10, 7, 6, 7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5730299db2c2fd14005689a7</td>\n",
              "      <td>how was vesey executed in 1822?</td>\n",
              "      <td>by 1820, charleston's population had grown to ...</td>\n",
              "      <td>382</td>\n",
              "      <td>hanged</td>\n",
              "      <td>15719</td>\n",
              "      <td>74</td>\n",
              "      <td>75</td>\n",
              "      <td>[43, 12, 25120, 2180, 5, 10201, 8]</td>\n",
              "      <td>[17, 9014, 2, 1908, 18, 103, 48, 2554, 7, 2106...</td>\n",
              "      <td>[[11, 8, 20], [20, 5, 9], [24, 3, 9, 3, 21], [...</td>\n",
              "      <td>[[22, 21], [27, 40, 29, 28], [23], [14, 11, 5,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ...                             char_tokenized_context\n",
              "0  572667e6708984140094c4f9  ...  [[5, 17, 4, 3, 10], [8, 24, 3, 10], [5], [13, ...\n",
              "1  56dec2483277331400b4d712  ...  [[9, 14, 11, 20, 5, 10, 39, 3, 7, 3, 19, 19, 3...\n",
              "2  5726e5995951b619008f81bb  ...  [[6, 4], [11, 5, 9], [22, 3, 3, 7], [8, 22, 9,...\n",
              "3  5726486f708984140094c157  ...  [[5, 17, 4, 3, 10], [10, 3, 4, 15, 10, 7, 6, 7...\n",
              "4  5730299db2c2fd14005689a7  ...  [[22, 21], [27, 40, 29, 28], [23], [14, 11, 5,...\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kw5Pk4c6I3S",
        "outputId": "54a73416-53d6-4c41-91b6-b693c5cc427c"
      },
      "source": [
        "print(tr_df['tokenized_question'].str.len().describe())\r\n",
        "vl_df['tokenized_question'].str.len().describe()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    70079.000000\n",
            "mean        11.273962\n",
            "std          3.715555\n",
            "min          1.000000\n",
            "25%          9.000000\n",
            "50%         11.000000\n",
            "75%         13.000000\n",
            "max         60.000000\n",
            "Name: tokenized_question, dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    17520.000000\n",
              "mean        11.333961\n",
              "std          3.753941\n",
              "min          1.000000\n",
              "25%          9.000000\n",
              "50%         11.000000\n",
              "75%         13.000000\n",
              "max         38.000000\n",
              "Name: tokenized_question, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsKYKsgR6-UK",
        "outputId": "adc66265-673c-40b6-eaf9-5538fd3e94e6"
      },
      "source": [
        "print(tr_df['tokenized_context'].str.len().describe())\r\n",
        "vl_df['tokenized_context'].str.len().describe()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    70079.000000\n",
            "mean       137.777323\n",
            "std         56.960478\n",
            "min         22.000000\n",
            "25%        102.000000\n",
            "50%        127.000000\n",
            "75%        164.000000\n",
            "max        766.000000\n",
            "Name: tokenized_context, dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    17520.000000\n",
              "mean       137.163071\n",
              "std         55.942698\n",
              "min         22.000000\n",
              "25%        102.000000\n",
              "50%        126.000000\n",
              "75%        163.000000\n",
              "max        766.000000\n",
              "Name: tokenized_context, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE3J6xJZ7Sqe"
      },
      "source": [
        "def len_words(dataset):\r\n",
        "  count_q = []\r\n",
        "  count_c = []\r\n",
        "\r\n",
        "  for idx, row in dataset.iterrows():\r\n",
        "    for w in row['char_tokenized_question']:\r\n",
        "      l = len(w)\r\n",
        "      count_q.append(l)\r\n",
        "      \r\n",
        "    for w in row['char_tokenized_context']:\r\n",
        "      m = len(w)\r\n",
        "      count_c.append(m)\r\n",
        "  \r\n",
        "  return pd.Series(count_q), pd.Series(count_c)\r\n",
        "\r\n",
        "t_q,t_c = len_words(tr_df)\r\n",
        "v_q,v_c = len_words(vl_df)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8R_-bUv9HGQ",
        "outputId": "bd99f9f5-e153-45fb-e046-3a882347ea82"
      },
      "source": [
        "print(t_q.describe())\r\n",
        "t_c.describe()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    790068.000000\n",
            "mean          4.447850\n",
            "std           2.677574\n",
            "min           1.000000\n",
            "25%           2.000000\n",
            "50%           4.000000\n",
            "75%           6.000000\n",
            "max          30.000000\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    9.655297e+06\n",
              "mean     4.626008e+00\n",
              "std      2.969615e+00\n",
              "min      1.000000e+00\n",
              "25%      2.000000e+00\n",
              "50%      4.000000e+00\n",
              "75%      7.000000e+00\n",
              "max      3.700000e+01\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lQQcXeQ9u_H",
        "outputId": "ab76d055-eb8e-4951-af2e-87e94c37037a"
      },
      "source": [
        "print(v_q.describe())\r\n",
        "v_c.describe()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    198571.000000\n",
            "mean          4.453077\n",
            "std           2.686618\n",
            "min           1.000000\n",
            "25%           2.000000\n",
            "50%           4.000000\n",
            "75%           6.000000\n",
            "max          24.000000\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2.403097e+06\n",
              "mean     4.629671e+00\n",
              "std      2.972735e+00\n",
              "min      1.000000e+00\n",
              "25%      2.000000e+00\n",
              "50%      4.000000e+00\n",
              "75%      7.000000e+00\n",
              "max      3.700000e+01\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC1K9nYs9ht2"
      },
      "source": [
        "There are obviously some outliers. We are compeled to get rid of some samples because of memory issues.\r\n",
        "\r\n",
        "We will get rid of contexts that have more than 300 words and questions that have more than 20 words.\r\n",
        "\r\n",
        "We will set the length of a word to 20 (in our dataset, the max length encoutered is 37)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WiAI_oA_3lY"
      },
      "source": [
        "QUESTION_MAXLEN = 20\r\n",
        "CONTEXT_MAXLEN = 300\r\n",
        "WORD_LEN = 20"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibILzhoh-Mf8"
      },
      "source": [
        "tr_df = tr_df[(tr_df['tokenized_question'].str.len() <= 20) & (tr_df['tokenized_context'].str.len() <= 300) & (tr_df['start'] <= 300) & (tr_df['end'] <= 300) ].reset_index(drop = True)\r\n",
        "vl_df = vl_df[(vl_df['tokenized_question'].str.len() <= 20) & (vl_df['tokenized_context'].str.len() <= 300) & (vl_df['start'] <= 300) & (vl_df['end'] <= 300) ].reset_index(drop = True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQTuvAo7oGcA",
        "outputId": "04452b3e-e162-4c7a-9e45-a088dd279250"
      },
      "source": [
        "tr_df.shape[0], vl_df.shape[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67482, 16854)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93mWQxFLG3z9",
        "outputId": "b4ca2b88-2da5-40dd-ed19-e139fa357412"
      },
      "source": [
        " print(f' we get rid of : {SAMPLES - (tr_df.shape[0] + vl_df.shape[0])} samples')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " we get rid of : 3263 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owvocjs5JCPa"
      },
      "source": [
        "with open('tokenizer.pickle', 'wb') as handle:\r\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "%mv tokenizer.pickle 'drive/MyDrive/NLP/data/' "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO51HjxuOb85"
      },
      "source": [
        "with open('char_tokenizer.pickle', 'wb') as handle:\r\n",
        "    pickle.dump(char_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "%mv char_tokenizer.pickle 'drive/MyDrive/NLP/data/' "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0O23CFlJInL"
      },
      "source": [
        "tr_padded_questions = tf.keras.preprocessing.sequence.pad_sequences(tr_df['tokenized_question'], padding = 'post', maxlen = QUESTION_MAXLEN)\r\n",
        "tr_padded_contexts = tf.keras.preprocessing.sequence.pad_sequences(tr_df['tokenized_context'], padding = 'post', maxlen = CONTEXT_MAXLEN)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjvD-ReFpsK9"
      },
      "source": [
        "np.save('drive/MyDrive/NLP/data/tr_padded_questions.npy', tr_padded_questions)\r\n",
        "np.save('drive/MyDrive/NLP/data/tr_padded_contexts.npy', tr_padded_contexts)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT8IxOUEJOX9"
      },
      "source": [
        "vl_padded_questions = tf.keras.preprocessing.sequence.pad_sequences(vl_df['tokenized_question'], padding = 'post', maxlen = QUESTION_MAXLEN)\r\n",
        "vl_padded_contexts = tf.keras.preprocessing.sequence.pad_sequences(vl_df['tokenized_context'], padding = 'post', maxlen = CONTEXT_MAXLEN)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLXyLCkwpwNC"
      },
      "source": [
        "np.save('drive/MyDrive/NLP/data/vl_padded_questions.npy', vl_padded_questions)\r\n",
        "np.save('drive/MyDrive/NLP/data/vl_padded_contexts.npy', vl_padded_contexts)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAc0OvPaQm61"
      },
      "source": [
        "pad_char_c = np.zeros((vl_df.shape[0], QUESTION_MAXLEN, WORD_LEN), dtype = np.int32)\r\n",
        "\r\n",
        "for i, value in vl_df['char_tokenized_question'].iteritems():\r\n",
        "  v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = WORD_LEN)\r\n",
        "  to_add = QUESTION_MAXLEN - v.shape[0]\r\n",
        "  add = np.zeros((to_add, WORD_LEN))\r\n",
        "  arr = np.vstack([v,add])\r\n",
        "  pad_char_c[i] = arr\r\n",
        "\r\n",
        "np.save('drive/MyDrive/NLP/data/vl_char_padded_questions.npy', pad_char_c)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7TPzCCzjDoX"
      },
      "source": [
        "pad_char_c = np.zeros((tr_df.shape[0], QUESTION_MAXLEN,WORD_LEN), dtype = np.int32)\r\n",
        "\r\n",
        "for i, value in tr_df['char_tokenized_question'].iteritems():\r\n",
        "  v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = WORD_LEN)\r\n",
        "  to_add = QUESTION_MAXLEN - v.shape[0]\r\n",
        "  add = np.zeros((to_add, WORD_LEN))\r\n",
        "  arr = np.vstack([v,add])\r\n",
        "  pad_char_c[i] = arr\r\n",
        "\r\n",
        "np.save('drive/MyDrive/NLP/data/tr_char_padded_questions.npy', pad_char_c)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoG-TYzd6saB"
      },
      "source": [
        "del pad_char_c # to free memory"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvSDAazhjPpt"
      },
      "source": [
        "pad_char_q = np.zeros((vl_df.shape[0], CONTEXT_MAXLEN, WORD_LEN), dtype = np.int32)\r\n",
        "\r\n",
        "for i, value in vl_df['char_tokenized_context'].iteritems():\r\n",
        "  v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = WORD_LEN)\r\n",
        "  to_add = CONTEXT_MAXLEN - v.shape[0]\r\n",
        "  add = np.zeros((to_add, WORD_LEN))\r\n",
        "  arr = np.vstack([v,add])\r\n",
        "  pad_char_q[i] = arr\r\n",
        "\r\n",
        "np.save('drive/MyDrive/NLP/data/vl_char_padded_contexts.npy', pad_char_q)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5wdii1-ln-o"
      },
      "source": [
        "pad_char_q =  np.zeros((tr_df.shape[0], CONTEXT_MAXLEN,WORD_LEN))\r\n",
        "\r\n",
        "for i, value in tr_df['char_tokenized_context'].iteritems():\r\n",
        "  v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = WORD_LEN)\r\n",
        "  to_add = CONTEXT_MAXLEN - v.shape[0]\r\n",
        "  add = np.zeros((to_add, WORD_LEN))\r\n",
        "  arr = np.vstack([v,add])\r\n",
        "  pad_char_q[i] = arr\r\n",
        "  \r\n",
        "np.save('drive/MyDrive/NLP/data/tr_char_padded_contexts.npy', pad_char_q)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1JBEoSr6w6I"
      },
      "source": [
        "del pad_char_q # to free memory"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNvYtxrbPKJs"
      },
      "source": [
        "num_classes = CONTEXT_MAXLEN\r\n",
        "y_start_train = tf.keras.utils.to_categorical(tr_df['start'].values, num_classes)\r\n",
        "y_end_train = tf.keras.utils.to_categorical(tr_df['end'].values, num_classes)\r\n",
        "\r\n",
        "y_start_valid = tf.keras.utils.to_categorical(vl_df['start'].values, num_classes)\r\n",
        "y_end_valid = tf.keras.utils.to_categorical(vl_df['end'].values, num_classes)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND79b1GXPSql"
      },
      "source": [
        "np.save('drive/MyDrive/NLP/data/tr_y_start.npy', y_start_train)\r\n",
        "np.save('drive/MyDrive/NLP/data/tr_y_end.npy', y_end_train)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGvrV6EnPj4g"
      },
      "source": [
        "np.save('drive/MyDrive/NLP/data/vl_y_start.npy', y_start_valid)\r\n",
        "np.save('drive/MyDrive/NLP/data/vl_y_end.npy', y_end_valid)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGAUj_UvNKSp"
      },
      "source": [
        "In fact we have created a character tokenizer but we won't work at the character level.  \r\n",
        "Indeed, we run oom when we try to work at this level.\r\n",
        "\r\n",
        "**EDIT** : it seems we can work at the character level if we significantly reduce the WORD_LEN (from 40 to 20) and only treat the first 199 tokens as real tokens and the others as UNK tokens"
      ]
    }
  ]
}